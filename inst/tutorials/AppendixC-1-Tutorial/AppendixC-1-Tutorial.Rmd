---
output: slidy_presentation
runtime: shiny_prerendered
title: "Appendix C \n\n Fundamentals of Mathematical Statistics"
author: "James Bang"
description: >
  This tutorial demonstrates the concepts of the laws of large numbers and central limit theorem.
---

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
gradethis::gradethis_setup()
knitr::opts_chunk$set(echo = FALSE)
```

## Law of Large Numbers

Weak Law of Large Numbers (Chebyshev) 

- As long as the variance of *X* does not increase "too fast" with *n*, then the probability that the sample mean differs from the *true* mean by a measureable amount converges to zero. 
- Formally, if: $$ \lim_{n \to \infty} \frac{1}{n^2} \sum_{i=1}^n{\sigma_i^2} = 0$$
$$ \lim_{n \to \infty} \Pr{(|\bar{x}_n-\mu|>\epsilon)} = 0$$ for any $\epsilon > 0$.

Strong Law of Large Numbers (Chebyshev) 

- As long as the standard error of $\bar{x}$ is always finite, then the probability that the sample mean *equals* the mean converges to one. 
- Formally, if: $\lim_{n \to \infty} \frac{\sigma_n}{\sqrt{n}} < \infty$
$$ \Pr(\lim_{n \to \infty} \bar{x}_n=\mu) = 1$$ for any $\epsilon > 0$.

## Simulating LLN

Let *X* be a random variable that follows a Poisson distribution with parameter $\lambda = 1$. 

- Create a vector taking values of 1 through 1000 and name it "size"
- Create a loop through various sample sizes, 1 through 1000 that calculates the mean of samples of sizes 1 through 1000 from a Poisson distribution and name it "xBar"
- Plot the values of xBar against size. 

```{r LLN, exercise=TRUE, exercise.eval=TRUE}
set.seed(8675309)
# When simulating random samples, it's a good idea to set a value for the random sead. 
# This helps make your work reproducible. 
xBar <- NULL # Create an empty object to store your xBars in before you run your loop. 


# I'm adding a red horizontal line to the plot for you - no charge! (Uncomment this line to run)
# abline(h = 1, col = 'red') 
```

```{r LLN-hint}
# When simulating random samples, it's a good idea to set a value for the random sead. 
# This helps make your work reproducible. 
set.seed(8675309)
# Define a NULL object to initiate your x-bars.
xBar <- NULL 
size <- c(...)
for(i in 1:1000) {            # A good way to execute repetitive commands is to run a "loop"
  xBar[i] = mean(rpois(...))  # Define each element in "xBar" 
}
plot(...)
# I'm adding a red horizontal line to the plot for you - no charge!
abline(h = 1, col = 'red') 
```

```{r LLN-solution}
set.seed(8675309)
xBar <- NULL 
size <- c(1:1000)
for(i in 1:1000) {
  xBar[i] = mean(rpois(n = size[i], lambda = 1))
}
plot(size, xBar, type = 'l')
abline(h = 1, col = 'red')
```

```{r LLN-check}
grade_code()
```

## Central Limit Theorem

For "sufficiently large" *n*, the distribution of the sample mean is normal with mean $\mu_x$ and standard deviation $\sigma_x/\sqrt{n}$. Formally, 

For any $\epsilon > 0$, there exists an $n_0$ such that for all $n > n_0$, 
$$|F(\frac{\bar{X}_n-\mu}{\sigma_x/\sqrt{n}}) - \Phi(\frac{\bar{X}_n-\mu}{\sigma_x/\sqrt{n}})| < \epsilon $$
for all points on *F*. 

<html>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">If someone says, &quot;Asymptotic properties are useless because we never have an infinite amount of data,&quot; please remind them that the purpose of asymptotic analysis is to approximate exact sampling properties.<br><br>And then knock them upside the head. (Not really.)<a href="https://twitter.com/hashtag/metricstotheface?src=hash&amp;ref_src=twsrc%5Etfw">#metricstotheface</a></p>&mdash; Jeffrey Wooldridge (@jmwooldridge) <a href="https://twitter.com/jmwooldridge/status/1378886894008819722?ref_src=twsrc%5Etfw">April 5, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</html>

## CLT Simulation

Let *X* be a random variable that follows a Poisson distribution with parameter $\lambda = 1$. 

- Create a matrix of distributions where each column represents 1000 simulations of the sample mean of a Poisson distribution of sizes $n = {1, 2, ..., 1000}$. (The first column consists of 1000 sample means with *n* = 1; the second consists of 1000 sample means with *n* = 2, and so on...)
- Use plot(density()) lot the smoothed estimated densities of xBar for n = 2, 10, 30, and 100, set the x-axis limits to (-1, 3) and the y-axis limits to (0, 4).

```{r CLT, exercise=TRUE, exercise.eval=TRUE}
set.seed(8675309)



```

```{r CLT-hint}
set.seed(8675309)
# Initialize a matrix with: 
  # The number of rows equal to the number of resamples of size *n* and 
  # The number of columns equal to the number of sample sizes. 
xBar <- matrix(NA, nrow = 1000, ncol = 1000) 
for(i in ...) {
  for(j in ...) {
    xBar[j, i] = ...
  }
}
plot(density(...), xlim = c(...), ylim = c(...))
lines(density(...))
lines(density(...))
lines(density(...))
```

```{r CLT-solution}
set.seed(8675309)
xBar <- matrix(NA, nrow = 1000, ncol = 1000)
size <- c(1:1000)
for(i in 1:1000) {
  for(j in 1:1000) {
    xBar[j, i] = mean(rpois(n = size[i], lambda = 1))
  }
}
plot(density(xBar[,2]), xlim = c(-1, 3), ylim = c(0, 4))
lines(density(xBar[,10]))
lines(density(xBar[,30]))
lines(density(xBar[,100]))
```

```{r CLT-check}
grade_code()
```

## Student's (Gosset's) t Distribution

If $\bar{x}$ is distributed normal, why do we test it on a t-distribution? 

- When $\sigma_x^2$ is unknown, the estimate, $\hat{\sigma_x^2}$ is a random variable
- The sample variance is distributed $\chi^2(1)$, which is basically the distribution of a square of a normal random variable (variance is based on $E(X^2)$). 
- Since $t = \frac{(\bar{x}-\mu)}{\sigma/\sqrt{n}}$, it is the *ratio* of a normal random variable ($\bar{x}$, minus a constant, $\mu$) and a $\chi^2(1)$ random variable ($\sigma^2$, divided by a constant, $\sqrt{n}$).  
- William Sealy Gossett (a.k.a. "Student") derived the distribution of this ratio, and calculated its probability tables for various sample sizes. 

## Fisher and Snedecor's F Distribution

An F-test (e.g. in ANalysis Of VAriance, ANOVA) tests the ratio of the sum of two variances

- The variance between groups; 
- The variance within groups. 

Both variances are distributed as $\chi^2$. 
- The variances in the numerator have $k$ degrees of freedom for the $k$ groups. 
- The variances in the denominator have $n-k$ degrees of freedom for the remaining information from the observations in the sample. 
- The resulting "F" distribution for the ratio has $k$ *numerator* degrees of freedom and $n-k$ *denominator* degrees of freedom. 

$$F = \frac{between variance}{within variance} ~ F(k, n-k)$$
