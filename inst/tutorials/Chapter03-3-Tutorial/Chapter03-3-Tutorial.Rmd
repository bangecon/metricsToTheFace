---
output: slidy_presentation
runtime: shiny_prerendered
title: "Chapter 3 \n\n Multiple Regression Analysis \n\n Gauss-Markov Theorem"
author: "James Bang"
description: >
  This tutorial introduces linear estimation with multiple variables.
---

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(tidyverse)
library(gganimate)
library(gifski)
library(ggthemes)
library(wooldridge)
wage1 <- wooldridge::wage1
wage.lm2 <- lm(wage ~ educ + exper, data = wage1)
wage1$sunspots <- rpois(length(wage1$wage), lambda = 10)
wage.lm3 <- lm(wage ~ educ + exper + sunspots, data = wage1)
wage1$white <- 1 - wage1$nonwhite
wage.lm4 <- lm(wage ~ educ + exper + white + nonwhite, data = wage1)
wage.lm5 <- lm(wage ~ educ + exper + white + nonwhite - 1, data = wage1)
gradethis::gradethis_setup()
knitr::opts_chunk$set(echo = FALSE)
```

## Omitted Variable Bias

Problem: if we misspecify our model, all bets are off! 

- Estimated model: $y = X_1\beta_1 + e$
- Correct model: $y = X_1\beta_1 + X_2\beta_2 + u$
- By the OLS assumptions, $E(Xu) = 0$ but $E(Xe)$ may not be!

$$ E(\tilde\beta_1) = E[(X_1'X_1)^{-1}X_1'(X_1\beta_1 + X_2\beta_2 + u)]$$  
$$ E(\tilde\beta_1) = \beta_1 + \beta_2E[(X_1'X_1)^{-1}X_1'X_2] $$  

We can even rewrite this as: 
$$ E(\hat\beta) = \beta + \beta_2\delta $$  
where $\delta$ is the coefficient from regressing $X_2$ on $X_1$, $X_2 = X_1\delta + e$. 

The presence of omitted variable bias requires: 

- $\beta_2 \ne 0$ (omitted variables, $X_2$ correlate with $y$) *and*
- $\delta \ne 0$ (omitted variables, $X_2$ correlate with $X_1$)

## Graphical Example using Partition Regression and a Dummy Variable (with Animation!)
```{r omittedVariableBiasDemo, echo = TRUE}
df <- data.frame(Z = as.integer((1:200>100))) %>%
  mutate(X =  .5   + 2*Z + rnorm(200)) %>%
  mutate(Y = -.5*X + 4*Z + 1 + rnorm(200),time="1") %>%
  group_by(Z) %>%
  mutate(mean_X=mean(X),mean_Y=mean(Y)) %>%
  ungroup()
#Calculate correlations
before_cor <- paste("1. Start with raw data. Correlation between X and Y: ",round(cor(df$X,df$Y),3),sep='')
after_cor <- paste("6. Analyze what's left! Correlation between X and Y controlling for Z: ",round(cor(df$X-df$mean_X,df$Y-df$mean_Y),3),sep='')
#Add step 2 in which X is demeaned, and 3 in which both X and Y are, and 4 which just changes label
dffull <- rbind(
  #Step 1: Raw data only
  df %>% mutate(mean_X=NA,mean_Y=NA,time=before_cor),
  #Step 2: Add x-lines
  df %>% mutate(mean_Y=NA,time='2. Figure out what differences in X are explained by Z'),
  #Step 3: X de-meaned
  df %>% mutate(X = X - mean_X,mean_X=0,mean_Y=NA,time="3. Remove differences in X explained by Z"),
  #Step 4: Remove X lines, add Y
  df %>% mutate(X = X - mean_X,mean_X=NA,time="4. Figure out what differences in Y are explained by Z"),
  #Step 5: Y de-meaned
  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=0,time="5. Remove differences in Y explained by Z"),
  #Step 6: Raw demeaned data only
  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=NA,time=after_cor))
p <- ggplot(dffull,aes(y=Y,x=X,color=as.factor(Z)))+geom_point()+
  geom_vline(aes(xintercept=mean_X,color=as.factor(Z)), na.rm = TRUE)+
  geom_hline(aes(yintercept=mean_Y,color=as.factor(Z)), na.rm = TRUE)+
  guides(color=guide_legend(title="Z"))+
  scale_color_colorblind()+
  labs(title = 'The Relationship between Y and X, Controlling for a Binary Variable Z \n{next_state}', caption="Graph Code Credit: Nick Huntington-Klein, 'causalgraphs' https://github.com/NickCH-K/causalgraphs")+
  transition_states(time,transition_length=c(12,32,12,32,12,12),state_length=c(160,100,75,100,75,160),wrap=FALSE)+
  ease_aes('sine-in-out')+
  exit_fade()+enter_fade()
animate(p,nframes=200, renderer = gifski_renderer())
```

## Irrelevant Variables? 

- Generate a completely random variable, sunspots, which takes values from a Poisson distribution with parameter $\lambda = 10$ , as a new column in *wage1*. 
- Regress *wage* on *educ*, *exper*, and *sunspots* (with a constant) and call this regression *wage.lm3*.
- Compare this to our previous regression, *wage.lm2* (which is identical but excludes *sunspots*) using stargazer(..., type = 'text').

```{r irrelevant, exercise = TRUE}


```

```{r irrelevant-hint}
# Always remember to set a seed before doing randomization! 
# This is science! It should be reproducible!
set.seed(8675309)
wage1$sunspots <- ...
wage.lm3 <- lm(..., ...)
stargazer(..., ..., type = 'text')
```

```{r irrelevant-solution}
set.seed(8675309)
wage1$sunspots <- rpois(length(wage1$wage), lambda = 10)
wage.lm3 <- lm(wage ~ educ + exper + sunspots, data = wage1)
stargazer(wage.lm2, wage.lm3, type = 'text')
```

```{r irrelevant-check}
grade_code("There's almost no difference! We do lose a degree of freedom, I guess?")
```

## A Caveat about Omitted Variables

```{r linearRegression, echo=FALSE}
question("Should tests for gender wage discrimination control for occupational choice?",
    answer("Yes", correct = TRUE, message = "You're right IF differences in occupation are NOT caused by gender discrimination."),
    answer("No", correct = TRUE, message = "You're right IF differences in occupation ARE caused by gender discrimination."),
    allow_retry = TRUE
  )
```

Sometimes it's helpful to draw the pattern of causality. There are some neat tools for this in the *ggdag* package. 

```{r ggdagDemo, echo = TRUE}


```

## Perfect Multicollinearity

When a group of variables are perfectly collinear (correlated), we cannot invert $X'X$. It becomes akin to dividing by zero. Example: White/Nonwhite. 

- Using the wage1 data create the variable *white* equal to one minus *nonwithe*. 
- Regress *wage* on *educ*, *exper*, *white* and *nonwhite*. Call this regression *wage.lm4* and summarize the results. 

```{r multicollinear, exercise = TRUE}


```

```{r multicollinear-hint}
wage1$white <- ...
wage.lm4 <- lm(..., ...)
summary(wage.lm4)
```

```{r multicollinear-solution}
wage1$white <- 1 - wage1$nonwhite
wage.lm4 <- lm(wage ~ educ + exper + white + nonwhite, data = wage1)
summary(wage.lm4)
```

```{r multicollinear-check}
grade_code("Notice how the lm() function drops *nonwhite* since it is a perfect linear function of *white* and the constant (reverse the order they appear and it will drop whichever is last).")
```

## Perfect Multicollinearity without a Constant

Suppose I want to know the absolute magnitude of the intercept for each group (white/nonwhite). 

- One way I could do this is by adding. The baseline for the omitted group (here, *nonwhite*) is the regular intercept. 
- Another way I could do this is to regress the model with both variables, excluding the intercept. 
Do this, name it *wage.lm5*, and summarize the results. 

```{r multicollinearNoConstant, exercise = TRUE}


```

```{r multicollinearNoConstant-hint}
wage.lm5 <- lm(..., ...)
summary(wage.lm5)
```

```{r multicollinearNoConstant-solution}
wage.lm5 <- lm(wage ~ educ + exper + white + nonwhite - 1, data = wage1)
summary(wage.lm5)
```

```{r multicollinearNoConstant-check}
grade_code("Notice that the coefficient for *nonwhite* is the same as the previous example's intercept; the value for *white* is the intercept of the previous regression plus its marginal effect for the *white* group.")
```

<html>
<blockquote class="twitter-tweet" data-partner="tweetdeck"><p lang="en" dir="ltr">I used to think that having smart statistical programs, such as Stata, recognize and drop perfectly collinear variables in OLS, 2SLS, FE, and so on was uniformly a good feature. But it can be lead to the wrong conclusion in complicated settings.<a href="https://twitter.com/hashtag/metricstotheface?src=hash&amp;ref_src=twsrc%5Etfw">#metricstotheface</a></p>&mdash; Jeffrey Wooldridge (@jmwooldridge) <a href="https://twitter.com/jmwooldridge/status/1383457780183420932?ref_src=twsrc%5Etfw">April 17, 2021</a></blockquote>
</html>

## Imperfect Multicollinearity

Variance Formula - Scalar Form

- Simple Regression Model
$$ Var(\hat\beta_j) = \frac{\sigma^2}{SST_x} $$
- Multiple Regression Model
$$ Var(\hat\beta_j) = \frac{\sigma^2}{SST_j(1-R_j^2)} = \frac{\sigma^2}{SST_j} \cdot V.I.F $$
  - $SST_j = \sum_{i=1}^n{(x_{ij} - \bar{x}_j)^2}$
  - $R_j^2$ is the $R^2$ obtained from regressing $x_j$ on all other $x'$s and a constant. 
  - The more correlated $x_j$ is with the other $x'$s, the more inflated the variance becomes.
  - Bias-variance trade-off.
