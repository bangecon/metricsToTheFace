---
output: 
  learnr::tutorial:
    progressive: true
runtime: shiny_prerendered
title: "Chapter 3 Multiple Regression Analysis - Gauss-Markov Theorem"
author: "James Bang"
description: "This tutorial simulates omitted variables bias and multicollinearity." 
---

<style type="text/css">
h1{font-size: 24pt}
h2{font-size: 20pt}
h3{font-size: 18pt}
h4,h5,h6{font-size: 16pt}
body{font-size: 16pt}
</style>

<script language="JavaScript">
$(function() {
   var editor;
   $('.ace_editor').each(function( index ) {
     editor = ace.edit(this);
     editor.setFontSize("16px");
   });
})
</script>

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(tidyverse)
library(gganimate)
library(gifski)
library(ggthemes)
library(stargazer)
library(ggdag)
library(wooldridge)
gradethis::gradethis_setup()
tutorial_options(exercise.eval = TRUE, exercise.reveal_solution = TRUE)
knitr::opts_chunk$set(echo = FALSE)
wage1 <- wooldridge::wage1
wage.lm2 <- lm(wage ~ educ + exper, data = wage1)
wage1$sunspots <- rpois(length(wage1$wage), lambda = 10)
wage.lm3 <- lm(wage ~ educ + exper + sunspots, data = wage1)
wage1$white <- 1 - wage1$nonwhite
wage.lm4 <- lm(wage ~ educ + exper + white + nonwhite, data = wage1)
wage.lm5 <- lm(wage ~ educ + exper + white + nonwhite - 1, data = wage1)
```

## Omitted Variable Bias

Problem: if we misspecify our model, all bets are off! 

1. Estimated model: $y = x_1\beta_1 + e$
2. Correct model: $y = x_1\beta_1 + x_2\beta_2 + u$
3. When we omit *relevant* variables $E(Xu) = 0$ but $E(Xe)$ isn't!

The presence of omitted variable bias requires: 

1. Omitted variables, $x_2$ correlate with $y$ *and*
2. Omitted variables, $x_2$ correlate with $x_1$

### Graphical Demonstration (with Animation!)

The following graph shows how omitting a single *relevant* variable can introduce quite a bit of bias. 

```{r omittedVariableBiasDemo}
df <- data.frame(Z = as.integer((1:200>100))) %>%
  mutate(X =  .5   + 2*Z + rnorm(200)) %>%
  mutate(Y = -.5*X + 4*Z + 1 + rnorm(200),time="1") %>%
  group_by(Z) %>%
  mutate(mean_X=mean(X),mean_Y=mean(Y)) %>%
  ungroup()
#Calculate correlations
before_cor <- paste("1. Start with raw data. Correlation between X and Y: ",round(cor(df$X,df$Y),3),sep='')
after_cor <- paste("6. Analyze what's left! Correlation between X and Y controlling for Z: ",round(cor(df$X-df$mean_X,df$Y-df$mean_Y),3),sep='')
#Add step 2 in which X is demeaned, and 3 in which both X and Y are, and 4 which just changes label
dffull <- rbind(
  #Step 1: Raw data only
  df %>% mutate(mean_X=NA,mean_Y=NA,time=before_cor),
  #Step 2: Add x-lines
  df %>% mutate(mean_Y=NA,time='2. Figure out what differences in X are explained by Z'),
  #Step 3: X de-meaned
  df %>% mutate(X = X - mean_X,mean_X=0,mean_Y=NA,time="3. Remove differences in X explained by Z"),
  #Step 4: Remove X lines, add Y
  df %>% mutate(X = X - mean_X,mean_X=NA,time="4. Figure out what differences in Y are explained by Z"),
  #Step 5: Y de-meaned
  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=0,time="5. Remove differences in Y explained by Z"),
  #Step 6: Raw demeaned data only
  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=NA,time=after_cor))
p <- ggplot(dffull,aes(y=Y,x=X,color=as.factor(Z)))+geom_point()+
  geom_vline(aes(xintercept=mean_X,color=as.factor(Z)), na.rm = TRUE)+
  geom_hline(aes(yintercept=mean_Y,color=as.factor(Z)), na.rm = TRUE)+
  guides(color=guide_legend(title="Z"))+
  scale_color_colorblind()+
  labs(title = 'The Relationship between Y and X, Controlling for a Binary Variable Z \n{next_state}', caption="Graph Code Credit: Nick Huntington-Klein, 'causalgraphs' https://github.com/NickCH-K/causalgraphs")+
  transition_states(time,transition_length=c(12,32,12,32,12,12),state_length=c(160,100,75,100,75,160),wrap=FALSE)+
  ease_aes('sine-in-out')+
  exit_fade()+enter_fade()
animate(p,nframes=200, renderer = gifski_renderer())
```

<details style="line-height:105%"><summary>Click here to see the code for creating the animated graph.</summary>

`df <- data.frame(Z = as.integer((1:200>100))) %>%`<br>
`  mutate(X =  .5   + 2*Z + rnorm(200)) %>%`<br>
`  mutate(Y = -.5*X + 4*Z + 1 + rnorm(200),time="1") %>%`<br>
`  group_by(Z) %>%`<br>
`  mutate(mean_X=mean(X),mean_Y=mean(Y)) %>%`<br>
`  ungroup()`<br>
# Calculate correlations <br>
`before_cor <- paste("1. Start with raw data. Correlation between X and Y: ",round(cor(df$X,df$Y),3),sep='')`<br>
`after_cor <- paste("6. Analyze what's left! Correlation between X and Y controlling for Z: ",`<br>
`  round(cor(df$X-df$mean_X,df$Y-df$mean_Y),3),sep='')`<br>
# Add step 2 in which X is demeaned, and 3 in which both X and Y are, and 4 which just changes label <br>
`dffull <- rbind(`<br>
  # Step 1: Raw data only <br>
`  df %>% mutate(mean_X=NA,mean_Y=NA,time=before_cor),`<br>
  #Step 2: Add x-lines <br>
`  df %>% mutate(mean_Y=NA,time='2. Figure out what differences in X are explained by Z'),`<br>
  #Step 3: X de-meaned <br>
`  df %>% mutate(X = X - mean_X,mean_X=0,mean_Y=NA,time="3. Remove differences in X explained by Z"),`<br>
  #Step 4: Remove X lines, add Y <br>
`  df %>% mutate(X = X - mean_X,mean_X=NA,time="4. Figure out what differences in Y are explained by Z"),`<br>
  #Step 5: Y de-meaned <br>
`  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=0,`<br>
`  time="5. Remove differences in Y explained by Z"),`<br>
  #Step 6: Raw demeaned data only<br>
`  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=NA,time=after_cor))`<br>
`p <- ggplot(dffull,aes(y=Y,x=X,color=as.factor(Z)))+geom_point()+`<br>
`  geom_vline(aes(xintercept=mean_X,color=as.factor(Z)), na.rm = TRUE)+`<br>
`  geom_hline(aes(yintercept=mean_Y,color=as.factor(Z)), na.rm = TRUE)+`<br>
`  guides(color=guide_legend(title="Z"))+`<br>
`  scale_color_colorblind()+`<br>
`  labs(title = 'The Relationship between Y and X, Controlling for a Binary Variable Z \n{next_state}',`<br>
`    caption="Graph Code Credit: Nick Huntington-Klein, 'causalgraphs'`<br>
`    https://github.com/NickCH-K/causalgraphs")+`<br>
`  transition_states(time,transition_length=c(12,32,12,32,12,12),state_length=c(160,100,75,100,75,160), `<br>
`    wrap=FALSE)+`<br>
`  ease_aes('sine-in-out')+`<br>
`  exit_fade()+enter_fade()`<br>
`animate(p,nframes=200, renderer = gifski_renderer())`<br>

</details>

### Mathematical Derivation

$$E(\tilde\beta_1) = E[(X_1'X_1)^{-1}X_1'(X_1\beta_1 + X_2\beta_2 + u)]$$  

$$ E(\tilde\beta_1) = \beta_1 + \beta_2E[(X_1'X_1)^{-1}X_1'X_2] $$  

We can rewrite this as: 

$$ E(\hat\beta_1) = \beta_1 + \beta_2\delta$$  

where $\delta$ is the coefficient from regressing $X_2$ on $X_1$, $X_2 = X_1\delta + e$. 

### Irrelevant Variables

1. Generate a completely random variable, `sunspots`, which takes values from a Poisson distribution with parameter $\lambda = 10$ , as a new column in *wage1*. 
2. Regress *wage* on *educ*, *exper*, and *sunspots* (with a constant) and call this regression *wage.lm3*.
3. Compare this to our previous regression, *wage.lm2* (which is identical but excludes *sunspots*) using stargazer(..., type = 'text').

```{r irrelevant, exercise = TRUE}
set.seed(8675309)

```

```{r irrelevant-hint}
# Always remember to set a seed before doing randomization! 
# This is science! It should be reproducible!
```

```{r irrelevant-solution}
set.seed(8675309)
wage1$sunspots <- rpois(length(wage1$wage), lambda = 10)
wage.lm3 <- lm(wage ~ educ + exper + sunspots, data = wage1)
stargazer(wage.lm2, wage.lm3, type = 'text')
```

```{r irrelevant-check}
grade_this({if (!inherits(.result, c("character"))) {
    fail("Your class of your answer should be character strings.")}
  if (.result[8] != "educ                       0.644***                0.642***        ") { 
    fail("The coefficients on education should be about 0.644 and 0.642, respectively") }
  pass()
})
```

### A Caveat about Omitted Variables

```{r linearRegression, echo=FALSE}
question("Should tests for gender wage discrimination control for occupational choice?",
    answer("Yes", correct = TRUE, message = "You're right IF differences in occupation are NOT caused by gender discrimination."),
    answer("No", correct = TRUE, message = "You're right IF differences in occupation ARE caused by gender discrimination."),
    allow_retry = TRUE
  )
```

Sometimes it's helpful to draw the pattern of causality. There are some neat tools for this in the *ggdag* package. 

```{r ggdagDemo, echo = TRUE}
wageGapDag  <- dagify(Wage ~ Educ + Gender + Occ,
  Occ ~ Educ + Gender)

ggdag(wageGapDag) 
ggdag_collider(wageGapDag)

```

## Multicollinearity

### Perfect Multicollinearity

When a group of variables are perfectly collinear (correlated), we cannot invert $X'X$. It becomes akin to dividing by zero. Example: White/Nonwhite. 

1. Using the wage1 data create the variable *white* equal to one minus *nonwithe*. 
2. Regress *wage* on *educ*, *exper*, *white* and *nonwhite*. Call this regression *wage.lm4* and summarize the results. 

```{r multicollinear, exercise = TRUE}


```

```{r multicollinear-hint}

```

```{r multicollinear-solution}
wage1$white <- 1 - wage1$nonwhite
wage.lm4 <- lm(wage ~ educ + exper + white + nonwhite, data = wage1)
summary(wage.lm4)
```

```{r multicollinear-check}

grade_this({if (!inherits(.result, c("summary.lm"))) {
    fail("Your class of your answer should be a `summary.lm` object")}
  if (round(.result$coefficients[4,1],4) != 0.0162) { 
    fail("The coefficient on the white variable should be about 0.016.") }
  pass()
})
```

### Perfect Multicollinearity without a Constant

Suppose I want to know the absolute magnitude of the intercept for each group (white/nonwhite). 

1. One way I could do this is by adding. The baseline for the omitted group (here, *nonwhite*) is the regular intercept. 
2. Another way I could do this is to regress the model with both variables, excluding the intercept. 

Do #2, name it *wage.lm5*, and summarize the results. 

```{r multicollinearNoConstant, exercise = TRUE}


```

```{r multicollinearNoConstant-hint}

```

```{r multicollinearNoConstant-solution}
wage.lm5 <- lm(wage ~ educ + exper + white + nonwhite - 1, data = wage1)
summary(wage.lm5)
```

```{r multicollinearNoConstant-check}
grade_this({if (!inherits(.result, c("summary.lm"))) {
    fail("Your class of your answer should be a `summary.lm` object")}
  if (.result$coefficients[4,1] != 3.40304) { 
    fail("The coefficient on the nonwhite variable should be about 0.016.") }
  pass()
})
```

### Imperfect Multicollinearity

Variance Formula - Scalar Form

Simple Regression Model

$$ Var(\hat\beta_j) = \frac{\sigma^2}{SST_x} $$

Multiple Regression Model

$$ Var(\hat\beta_j) = \frac{\sigma^2}{SST_j(1-R_j^2)} = \frac{\sigma^2}{SST_j} \cdot V.I.F $$

Effect of Multicollinearity
  1. $SST_j = \sum_{i=1}^n{(x_{ij} - \bar{x}_j)^2}$
  2. $R_j^2$ is the $R^2$ obtained from regressing $x_j$ on all other $x'$s and a constant. 
  3. The more correlated $x_j$ is with the other $x'$s, the more inflated the variance becomes.

Bias-variance trade-off.
