---
output: 
  learnr::tutorial: 
    progressive: true
runtime: shiny_prerendered
title: "Chapter 3: Multiple Regression Analysis - Estimation"
author: "James Bang"
description: >
  This tutorial introduces linear estimation with multiple variables.
---

<style type="text/css">
h1{font-size: 24pt}
h2{font-size: 20pt}
h3{font-size: 18pt}
h4,h5,h6{font-size: 16pt}
body{font-size: 16pt}
</style>

<script language="JavaScript">
$(function() {
   var editor;
   $('.ace_editor').each(function( index ) {
     editor = ace.edit(this);
     editor.setFontSize("16px");
   });
})
</script>

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(stargazer)
library(xfun)
library(htmltools)
library(wooldridge)
tutorial_options(exercise.eval = TRUE, exercise.reveal_solution = TRUE)
gradethis::gradethis_setup()
knitr::opts_chunk$set(echo = FALSE)
wage1 <- wooldridge::wage1
wage.lm1 <- lm(wage ~ educ, data = wage1)
wage.lm2 <- lm(wage ~ educ + exper, data = wage1)
```

## Multiple Regression

### Wages and Education AND MORE!

- Using the wage1 data, regress wages on education (with an intercept). 

Name this wage.lm1

- Using the wage1 data, regress wages on education and experience (with an intercept). 
- Name this wage.lm2
- Summarize each regression object using "summary()"

```{r multipleLM, exercise = TRUE}


```

```{r multipleLM-hint}
# The "lm" function requires two main arguments: function (y ~ x1 + x2...) and data (a data frame) 
```

```{r multipleLM-solution}
wage.lm1 <- lm(wage ~ educ, data = wage1)
wage.lm2 <- lm(wage ~ educ + exper, data = wage1)
summary(wage.lm1)
summary(wage.lm2)
```

```{r multipleLM-check}
grade_this({if (!inherits(.result, c("summary.lm"))) {
    fail("Your class of your answer should be a summary of an lm object.")}
  if (round(.result$coefficients[2],4) != 0.6443) { 
    fail("The coefficient on education should be about 0.644.") }
  pass()
})
```

### Table Output for Regressions

- Display a *stargazer* object of each of the previous regressions using the *stargazer* function in the *stargazer* package. 
- Display the output without assigning it to a named object 
- Use a *text* output type. (In general, it's more useful to use $type = \text{'html'}$ and $out = \text{'filename.html'}$)

```{r gtLM, exercise = TRUE}

```

```{r gtLM-hint}
# Use the `type = 'text'` option to display a readable text table. 
```

```{r gtLM-solution}
stargazer(wage.lm1, wage.lm2, type = 'text')
```

```{r gtLM-check}
grade_this({if (!inherits(.result, c("character"))) {
    fail("Your class of your answer should be a list of character strings.")}
  if (length(.result) != 24) { 
    fail("The length of the result should equal the number of rows in your table (including separators and text)" ) }
  if (.result[8] != "educ                        0.541***                0.644***        "){ 
    fail("The edycation coefficients don't look quite right.") }
  pass()
})
```

### Raw Stargazer HTML Output 

```{r, echo = TRUE}
stargazer(wage.lm1, wage.lm2, type = 'html')
```

### Formatted Stargazer HTML Output 

You can directly insert a `stargazer` table in an `html` Rmarkdown document. One is to include the command that generates the `html` in an `R` chunk (with the option `results='asis'`) that itself is inside an `html` code chunk.  

```{r, echo = TRUE, eval=FALSE}
stargazer(
  wage.lm1,
  wage.lm2,
  title = "Education and Wages",
  type = "html",
  float = TRUE,
  no.space = TRUE,
  header = FALSE,
  covariate.labels = c("Education", "Experience")
)
```

```{=html}
```{r, results='asis'}
stargazer(
  wage.lm1,
  wage.lm2,
  title = "Education and Wages",
  type = "html",
  float = TRUE,
  no.space = TRUE,
  header = FALSE,
  covariate.labels = c("Education", "Experience")
)
```
<br>
```

Alternately, you might want to run your code from a `R` source file, save your pretty tables as html files, and include the content of those files in your `html` (or other) document. 

One way to achieve this is to insert a code chunk with the `htmltools::includeHTML()` function: 

```{r results='asis'}
includeHTML('output/wage.html')
```
<br>

Equivalently, you could do the same thing using the `xfun::file_string()` function: 

```{r echo = TRUE, results='asis'}
file_string('output/wage.html')
```
<br>

Since this tutorial knits as an `html` document I needed to use some extra tricks. More commonly, you would knit your document as a `pdf`, and in this case you would want to leave out the `html` wrap, and set `type = "latex"`. 

## Implementing Regression

### Assumptions of the Classical Regression Model

1. Linear in the parameters
2. Random Sampling: $(X_i, Y_i)$ are independently and identically distributed
3. No Perfect Multicollinearity
4. $E[u|x] = 0$
5. Homoskedasticity: $u_i{'s}$ have constant variance regardless of the value of $x$
6. Large Outliers are Unlikely: $X$ and $Y$ have finite fourth moments
$$E(X^4)< \infty$$
$$E(Y^4)< \infty$$

### Deriving the OLS Estimator

One variable

$$ \min_{\hat\beta_0,\hat\beta_1} \sum_{i=1}^n{(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1})^2} $$

Two variables

$$ \min_{\hat\beta_0,\hat\beta_1,\hat\beta_2} \sum_{i=1}^n{(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\hat{\beta}_2x_{i2})^2} $$

$k$ variables

$$ \min_{\hat\beta_0,\hat\beta_1,...,\hat\beta_k} \sum_{i=1}^n{(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-...-\hat{\beta}_kx_{ik})^2} $$

Taking the derivative with respect to each of the $\hat{\beta}$'s separately yields a system of equations equal to the number of $\hat{\beta}$'s {$k + 1$). 

### Interpretating the Coefficients

Predicted values of y:
$$\hat{wage}=\hat{\beta}_0+\hat{\beta}_1educ+\hat{\beta}_2exper$$
Changes in $\hat{y}$:
$$ \Delta \hat{wage}=\hat{\beta}_0+\hat{\beta}_1 \Delta educ + \hat{\beta}_2 \Delta exper$$
*Ceteris paribus* the predicted change in $y$ in response to an observed change in $x$ is:
$$ \Delta \hat{wage}=0+\hat{\beta}_1 \Delta educ+\hat{\beta}_2 (0)$$
$$ \Delta \hat{wage}=\hat{\beta}_1 \Delta educ $$
The predicted change in wage is $\hat\beta_1$ dollars per hour *for each additional year of education*. 

### Optimization by Hand Demo

The following code very closely replicates the method used in the $lm$ function for minimizing the sum of squared residuals.

```{r optimDemo, echo = TRUE}
ssr <- function(b, y, x1, x2) {
  b1 <- b[1]
  b2 <- b[2]
  b0 <- b[3]
  sum((y - b0 - b1 * x1 - b2 * x2) ^ 2)
}
b.ols <-
  optim(
    par = c(mean(wage1$wage), 0, 0),
    fn = ssr,
    method = "BFGS",
    y = wage1$wage,
    x1 = wage1$educ,
    x2 = wage1$exper
  )
b.ols$par
sqrt(b.ols$value / (length(wage1$wage) - length(b.ols$par)))
```

Note: the value of the function at the minimum is the SSR, so $\hat\sigma^2 = b.ols\$value/(n-k)$

## Partition Regression

### Partialing Out Control Variables

Regress wages on experience and then education on experience. 

Summarize the auxiliary regression of education. 

```{r partials, exercise = TRUE, exercise.reveal_solution = FALSE}


```

```{r partials-hint}
# The "lm" function requires two main arguments: function (y ~ x1 + x2...) and data (a data frame) 
```

```{r partials-solution, exercise.reveal_solution = FALSE}
wage.p <- lm(wage~exper, data = wage1)
educ.p <- lm(educ~exper, data = wage1)
summary(educ.p)
```

```{r partials-check}
grade_this({if (!inherits(.result, c("summary.lm"))) {
    fail("Your class of your answer should be a summary of an lm object.")}
  if (round(.result$coefficients[2],4) != -0.0611) { 
    fail("The coefficient on experience should be about -0.06.") }
  pass()
})
```

### Regressing the Residuals

Summarize the regression of the residuals of these regressions on each other and compare the effects of education to the summary of $wage.lm2$. 

```{r residReg, exercise = TRUE}


```

```{r residReg-hint}
# We do not need to store the argument. We can just nest or pipe the lm() function with summary()
```

```{r residReg-solution, exercise.reveal_solution = FALSE}
summary(lm(wage.p$resid~educ.p$residuals))
summary(wage.lm2)
```

```{r residReg-check}
grade_this({if (!inherits(.result, c("summary.lm"))) {
    fail("Your class of your answer should be a summary of an lm object.")}
  if (round(.result$coefficients[2],4) != 0.6443) { 
    fail("The coefficient on education should be about 0.644.") }
  pass("Notice that the coefficients are identical! The standard error of regression (and hence the standard errors of the coeffients, t-statistics, and p-values) differ slightly. This is because R only takes into account the education variable when determining the degrees of freedom. The true standard error of the residual regression should take that into account and the SER should be scaled up by $(n-2)/(n-k)$, where $k = 3$ in this example.")
})
```

## Properties of OLS Redux

- $\sum_{i=1}^{n}{\hat{u}_i = 0}$
- $\sum_{i=1}^{n}{x_i\hat{u_i} = 0}$
- The *Total* Sum of Squares, $SST = \sum_{i=1}^n{(y_i-\bar{y})^2}$
- The *Explained* Sum of Squares, $SSE = \sum_{i=1}^n{(\hat{y}_i-\bar{y})^2}$
- The *Residual* Sum of Squares, $SSR = \sum_{i=1}^n{(y_i-\hat{y})^2}$
- SST = SSE + SSR (see section 2.3 for proof)
- Goodness of Fit, $R^2 = SSE/SST = 1 - SSR/SST$

We will discuss issues with using the (unadjusted) $R^2$ more in Chapter 6. 
