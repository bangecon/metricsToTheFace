---
output: slidy_presentation
runtime: shiny_prerendered
title: "Chapter 3 \n\n Multiple Regression Analysis - Estimation"
author: "James Bang"
description: >
  This tutorial introduces linear estimation with multiple variables.
---

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(stargazer)
library(wooldridge)
wage1 <- wooldridge::wage1
wage.lm1 <- lm(wage ~ educ, data = wage1)
wage.lm2 <- lm(wage ~ educ + exper, data = wage1)
gradethis::gradethis_setup()
knitr::opts_chunk$set(echo = FALSE)
```

## A Simple Regression Redux: Wages and Education AND MORE!

- Using the wage1 data, regress wages on education (with an intercept). 

Name this wage.lm1

- Using the wage1 data, regress wages on education and experience (with an intercept). 
- Name this wage.lm2
- Summarize each regression object using "summary()"

```{r multipleLM, exercise = TRUE}


```

```{r multipleLM-hint}
# The "lm" function requires two main arguments: function (y ~ x1 + x2...) and data (a data frame) 
wage.lm1 <- lm(..., ...)
wage.lm2 <- lm(..., ...)
summary(...)
summary(...)
```

```{r multipleLM-solution}
wage.lm1 <- lm(wage ~ educ, data = wage1)
wage.lm2 <- lm(wage ~ educ + exper, data = wage1)
summary(wage.lm1)
summary(wage.lm2)
```

```{r multipleLM-check}
grade_code()
```

## Table Output for Nested Regressions

- Display a *stargazer* object of each of the previous regressions using the *stargazer* function in the *stargazer* package. 
- Display the output without assigning it to a named object 
- Use a *text* output type. (In general, it's more useful to use $type = \text{'html'}$ and $out = \text{'filename.html'}$)

```{r gtLM, exercise = TRUE}

```

```{r gtLM-hint}
stargazer(..., ..., type = 'text')
```

```{r gtLM-solution}
stargazer(wage.lm1, wage.lm2, type = 'text')
```

```{r gtLM-check}
grade_code()
```

## Stargazer Output (using $type = \text{'html'}$)

<html>
<table style="text-align:center"><tr><td colspan="3" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"></td><td colspan="2"><em>Dependent variable:</em></td></tr>
<tr><td></td><td colspan="2" style="border-bottom: 1px solid black"></td></tr>
<tr><td style="text-align:left"></td><td colspan="2">wage</td></tr>
<tr><td style="text-align:left"></td><td>(1)</td><td>(2)</td></tr>
<tr><td colspan="3" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">educ</td><td>0.541<sup>***</sup></td><td>0.644<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.053)</td><td>(0.054)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td></tr>
<tr><td style="text-align:left">exper</td><td></td><td>0.070<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td></td><td>(0.011)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td></tr>
<tr><td style="text-align:left">Constant</td><td>-0.905</td><td>-3.391<sup>***</sup></td></tr>
<tr><td style="text-align:left"></td><td>(0.685)</td><td>(0.767)</td></tr>
<tr><td style="text-align:left"></td><td></td><td></td></tr>
<tr><td colspan="3" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">Observations</td><td>526</td><td>526</td></tr>
<tr><td style="text-align:left">R<sup>2</sup></td><td>0.165</td><td>0.225</td></tr>
<tr><td style="text-align:left">Adjusted R<sup>2</sup></td><td>0.163</td><td>0.222</td></tr>
<tr><td style="text-align:left">Residual Std. Error</td><td>3.378 (df = 524)</td><td>3.257 (df = 523)</td></tr>
<tr><td style="text-align:left">F Statistic</td><td>103.363<sup>***</sup> (df = 1; 524)</td><td>75.990<sup>***</sup> (df = 2; 523)</td></tr>
<tr><td colspan="3" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"><em>Note:</em></td><td colspan="2" style="text-align:right"><sup>*</sup>p<0.1; <sup>**</sup>p<0.05; <sup>***</sup>p<0.01</td></tr>
</table>
<\html>

## Assumptions of the Classical Regression Model

- Linear in the parameters
- Random Sampling: $(X_i, Y_i)$ are independently and identically distributed
- No Perfect Multicollinearity
- E[u|x] = 0
- Homoskedasticity: $u_i{'s}$ have constant variance regardless of the value of $x$
- Large Outliers are Unlikely: $X$ and $Y$ have finite fourth moments
$$E(X^4)< \infty$$
$$E(Y^4)< \infty$$

<html>
<blockquote class="twitter-tweet" data-partner="tweetdeck"><p lang="en" dir="ltr">As a follow-up, I don&#39;t believe in laying out all assumptions at once. I only make an assumption when it&#39;s needed for a result. This way, students are forced to learn which assumps are used to obtain which results. It pays dividends later.</p>&mdash; Jeffrey Wooldridge (@jmwooldridge) <a href="https://twitter.com/jmwooldridge/status/1405604458432798721?ref_src=twsrc%5Etfw">June 17, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</html>

Oops! Looks like I've disappointmented to our econometric sensei!

## Implementing Regression

Least Squares Redux

- One variable
$$ \min_{\hat\beta_0,\hat\beta_1} \sum_{i=1}^n{(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1})^2} $$
- Two variables
$$ \min_{\hat\beta_0,\hat\beta_1,\hat\beta_2} \sum_{i=1}^n{(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\hat{\beta}_2x_{i2})^2} $$
- $k$ variables
$$ \min_{\hat\beta_0,\hat\beta_1,...,\hat\beta_k} \sum_{i=1}^n{(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-...-\hat{\beta}_kx_{ik})^2} $$
Taking the derivative with respect to each of the ${\beta}'s$ separately yields a system of equations equal to the number of ${\beta}\text{'s }(k + 1)$ and an equal number of parameters to solve for. 

## Interpretation

Predicted values of y:
$$\hat{wage}=\hat{\beta}_0+\hat{\beta}_1educ+\hat{\beta}_2exper$$
Changes in $\hat{y}$:
$$ \Delta \hat{wage}=\hat{\beta}_0+\hat{\beta}_1 \Delta educ + \hat{\beta}_2 \Delta exper$$
*Ceteris paribus* the predicted change in $y$ in response to an observed change in $x$ is:
$$ \Delta \hat{wage}=0+\hat{\beta}_1 \Delta educ+\hat{\beta}_2 (0)$$
$$ \Delta \hat{wage}=\hat{\beta}_1 \Delta educ $$
The predicted change in wage is $\hat\beta_1$ dollars per hour *for each additional year of education*. 

## Optimization by Hand Demo

The following code very closely replicates the method used in the $lm$ function for minimizing the sum of squared residuals.

```{r optimDemo, echo = TRUE}
ssr <- function(b, y, x1, x2) {
    b1 <- b[1]
    b2 <- b[2]
    b0 <- b[3]
    sum((y - b0 - b1*x1 - b2*x2)^2)
}
b.ols <- optim(par = c(mean(wage1$wage),0,0), fn = ssr, method = "BFGS", y = wage1$wage, x1 = wage1$educ, x2 = wage1$exper)
b.ols$par
sqrt(b.ols$value/(length(wage1$wage)-length(b.ols$par)))
```

Note: the value of the function at the minimum is the SSR, so $\hat\sigma^2 = sqrt(b.ols\$value/(n-k))$

## Partialing Out Control Variables

Regress wages on experience and then education on experience. 

Name the results $wage.p$ and $educ.p$

```{r partials, exercise = TRUE}


```

```{r partials-hint}
# The "lm" function requires two main arguments: function (y ~ x1 + x2...) and data (a data frame) 
wage.p <- lm(..., ...)
educ.p <- lm(..., ...)
```

```{r partials-solution}
wage.p <- lm(wage~exper, data = wage1)
educ.p <- lm(educ~exper, data = wage1)
```

```{r partials-check}
grade_code()
```

Summarize the regression of the residuals of these regressions on each other and compare the effects of education to the summary of $wage.lm2$. 

```{r residReg, exercise = TRUE}


```

```{r residReg-hint}
# We do not need to store the argument. We can just nest the lm() function inside summary()
summary(lm(..., ...))
summary(...)
```

```{r residReg-solution}
summary(lm(wage.p$resid~educ.p$residuals))
summary(wage2.lm2)
```

```{r residReg-check}
grade_code("Notice that the coefficients are identical! The standard error of regression (and hence the standard errors of the coeffients, t-statistics, and p-values) differ slightly. This is because R only takes into account the education variable when determining the degrees of freedom. The true standard error of the residual regression should take that into account and the SER should be scaled up by $(n-2)/(n-k)$, where $k = 3$ in this example.")
```

## Properties of OLS Redux

- $\sum_{i=1}^{n}{\hat{u}_i = 0}$
- $\sum_{i=1}^{n}{x_i\hat{u_i} = 0}$
- The *Total* Sum of Squares, $SST = \sum_{i=1}^n{(y_i-\bar{y})^2}$
- The *Explained* Sum of Squares, $SSE = \sum_{i=1}^n{(\hat{y}_i-\bar{y})^2}$
- The *Residual* Sum of Squares, $SSR = \sum_{i=1}^n{(y_i-\hat{y})^2}$
- SST = SSE + SSR (see section 2.3 for proof)
- Goodness of Fit, $R^2 = SSE/SST = 1 - SSR/SST$

We will discuss issues with using the (unadjusted) $R^2$ more in Chapter 6. 

<html>
<blockquote class="twitter-tweet" data-partner="tweetdeck"><p lang="en" dir="ltr">If I had my way, any R-squared reported with linear estimation would be computed only after we net out factors that required no imagination our our part. In time series, y(t) would be detrended and/or deasonalized -- depending on what&#39;s being controlled for.<a href="https://twitter.com/hashtag/metricstotheface?src=hash&amp;ref_src=twsrc%5Etfw">#metricstotheface</a></p>&mdash; Jeffrey Wooldridge (@jmwooldridge) <a href="https://twitter.com/jmwooldridge/status/1374502138714279949?ref_src=twsrc%5Etfw">March 23, 2021</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</html>
