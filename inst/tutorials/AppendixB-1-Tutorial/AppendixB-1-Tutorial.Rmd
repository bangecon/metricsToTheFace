---
output: 
  learnr::tutorial:
    progressive: true
title: "Appendix B Fundamentals of Probability" 
author: Jim Bang
description: "This tutorial reviews some basic concepts about probability and probability distributions."
---

<style type="text/css">
h1{font-size: 24pt}
h2{font-size: 20pt}
h3{font-size: 18pt}
h4,h5,h6{font-size: 16pt}
body{font-size: 16pt}
</style>

<script language="JavaScript">
$(function() {
   var editor;
   $('.ace_editor').each(function( index ) {
     editor = ace.edit(this);
     editor.setFontSize("16px");
   });
})
</script>

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
ceosal1 <- wooldridge::ceosal1
affairs <- wooldridge::affairs
attach(ceosal1) 
E_sal.roe <- mean(salary*roe)
E_sal.E_roe <- mean(salary)*mean(roe)
affairs$haskids <- factor(affairs$kids, labels = c("no","yes"))
affairs$marriage <- factor(affairs$ratemarr, labels = c("very unhappy","unhappy","average","happy", "very happy"))
gradethis::gradethis_setup()
knitr::opts_chunk$set(echo = FALSE)
```

## Probability Distributions 

### Probability Mass Functions of Discrete Random Variables

Probability mass function, $Pr(x)$, maps all possible values of $X$ into the probabilities that $X = x$.

+----------------+--------------------------+--------------------------+--------------------------+--------------------------+
| Name           | PMF [$Pr(x=x)$]          | CDF [$P(x<=x)$]          | Quantile [$P^{-1}(p)$]   | Random \#s               |
+================+:========================:+:========================:+:========================:+:========================:+
| Bernoulli      | dbinom(x, 1, $\pi_s$)    | pbinom(x, 1, $\pi_s$)    | qbinom(x, 1, $\pi_s$)    | rbinom(R, 1, $\pi_s$)    |
+----------------+--------------------------+--------------------------+--------------------------+--------------------------+
| Binomial       | dbinom(x, *n*, $\pi_s$)  | pbinom(x, *n*, $\pi_s$)  | qbinom(p, *n*, $\pi_s$)  | rbinom(R, *n*, $\pi_s$)  |
+----------------+--------------------------+--------------------------+--------------------------+--------------------------+
| Hypergeometric | dhyper(x, *S*, *F*, *n*) | phyper(x, *S*, *F*, *n*) | qhyper(p, *S*, *F*, *n*) | rhyper(R, *S*, *F*, *n*) |
+----------------+--------------------------+--------------------------+--------------------------+--------------------------+
| Poisson        | dpois(x, $\lambda$)      | dpois(x, $\lambda$)      | qpois(p, $\lambda$)      | rpois(R, $\lambda$)      |
+----------------+--------------------------+--------------------------+--------------------------+--------------------------+
| Geometric      | dgeom(x, $\pi_s$)        | dgeom(x, $\pi_s$)        | qgeom(p, $\pi_s$)        | rgeom(R, $\pi_s$)        |
+----------------+--------------------------+--------------------------+--------------------------+--------------------------+

: Discrete Probability Distributions in R: x is the value of the variable; p is the cumulative probability; and R is the number of random draws. Other arguments represent parameters of the distribution.

\newpage
### Probability Density Functions of Continuous Random Variables

Probability density function, $f(x)$, measures how tightly packed the probability is around $x$, *not* the probability at that point ($Pr[X=x]=0$).

+---------------+----------------------------+----------------------------+----------------------------+----------------------------+
| Name          | PMF [$f(x)$]               | CDF [$P(x<=x)$]            | Quantile [$P^{-1}(p)$]     | Random \#s                 |
+===============+:==========================:+:==========================:+:==========================:+:==========================:+
| Uniform       | dunif(x, min, max)         | punif(x, min, max)         | qunif(p, min, max)         | runif(R, min, max)         |
+---------------+----------------------------+----------------------------+----------------------------+----------------------------+
| Logistic      | dlogis(x, $\mu$)           | plogis(x, $\mu$)           | qlogis(p, $\mu$)           | rlogis(R, $\mu$)           |
+---------------+----------------------------+----------------------------+----------------------------+----------------------------+
| Exponential   | dexp(x, $\lambda$)         | pexp(x, $\lambda$)         | qexp(p, $\lambda$)         | rexp(R, $\lambda$)         |
+---------------+----------------------------+----------------------------+----------------------------+----------------------------+
| Std. Normal   | dnorm(x, 0, 1)             | pnorm(x, 0, 1)             | qnorm(p, 0, 1)             | rnorm(R, 0, 1)             |
+---------------+----------------------------+----------------------------+----------------------------+----------------------------+
| Normal        | dnorm(x, $\mu$, $\sigma$)  | pnorm(x, $\mu$, $\sigma$)  | qnorm(p, $\mu$, $\sigma$)  | rnorm(R, $\mu$, $\sigma$)  |
+---------------+----------------------------+----------------------------+----------------------------+----------------------------+
| Lognormal     | dlnorm(x, $\mu$, $\sigma$) | plnorm(x, $\mu$, $\sigma$) | qlnorm(p, $\mu$, $\sigma$) | rlnorm(R, $\mu$, $\sigma$) |
+---------------+----------------------------+----------------------------+----------------------------+----------------------------+
| $\chi^2$      | dchisq(x, n)               | pchisq(x, n)               | qchisq(p, n)               | rchisq(R, n)               |
+---------------+----------------------------+----------------------------+----------------------------+----------------------------+
| Student's *t* | dt(x, *df*)                | dt(x, *df*)                | qt (p, *df*)               | rt(R, *df*)                |
+---------------+----------------------------+----------------------------+----------------------------+----------------------------+
| *F*           | df(x, *df*~1~, *df*~2~)    | pf(x, *df*~1~, *df*~2~)    | qf(p, *df*~1~, *df*~2~)    | rf(R, *df*~1~, *df*~2~)    |
+---------------+----------------------------+----------------------------+----------------------------+----------------------------+

: Coninuous Probability Distributions in R: x is the value of the variable; p is the cumulative probability; and R is the number of random draws. Other arguments represent parameters of the distribution.

\newpage
### Cumulative Probability Distribution of a Random Variable

Cumulative probabilities for a discrete R.V.

$$P(X \le x^*)= \sum_{x=-\infty}^{x^*}p(x)$$

Cumulative probabilities for a continuous R.V.

$$P(X \le x^*)= F(x^*) = \int_{-\infty}^{x^*}f(x)dx$$ = The area under f to the left of x.

$$P(X = x^*) = \int_{x^*}^{x^*}f(x)dx = F(x^*) - F(x^*) = 0$$

\newpage
## Expected Value 

The first moment of *X* is $E[X]$, or $\mu$. 

### Discrete Random Variable

$$E(X)= \sum_{x=-\infty}^{\infty}xp(x)= \vec{x} \cdot \vec{p}(x)$$

For a random variable, y, defined on a uniform *discrete* distribution, py, from zero to ten, calculate $E(y)$.

```{r discreteEz, exercise = TRUE}

```

```{r discreteEz-solution}
y <- c(0:10)
py <- rep(1/length(y), length(y))
y%*%py
```

```{r discreteEz-check}
grade_code()
```

### Continuous Random Variable

$$E(X) = \int_{-\infty}^{\infty}xf(x)dx$$

For a random variable, x, defined on a uniform *continuous* distribution between zero and ten, calculate $E(x)$.

```{r continuousEx, exercise = TRUE}

```

```{r continuousEx-solution}
integrate(function(x) x/10, 0, 10)
```

```{r continuousEx-check}
grade_code()
```

Notice that $E(X^2) > E(X)^2$. This because x^2 is a convex function, and this property is known as Jensen’s Inequality.

### Expected Value of a Function of a Random Variable

Linear functions and expectation

1. Constants can factor out of the expectation: $E(c) = c$ for any constant, $c$. 
2. Addition is separable inside expectation: $E(X + Y) = E(X) + E(Y)$. 
3. For any *linear* function, $f$, $E[f(X)] = f[E(X)]$, i.e. $E(a + bX) = a + bE(X)$ for any constants, $a$ and $b$.
4. For any *convex* function, $f$, $E[f(X)] > f[E(X)]$, e.g. $E(X^2) > E(X)^2$; <br>
   For any *concave* function, $f$, $E[f(X)] < f[E(X)]$, e.g. $E[ln(X)] < ln[E(X)]$. 

For a random variable, x, defined on a uniform *continuous* distribution between zero and ten, calculate $E(x)^2$ and $E(x^2)$.

```{r JensensInequality, exercise = TRUE}

```

```{r JensensInequality-solution}
# Calculate E(x), E(X)^2, and E(X^2)
integrate(function(x) x/10, 0, 10)$value^2
integrate(function(x) (x^2)/10, 0, 10)$value
```

```{r JensensInequality-check}
grade_code()
```

### Mean, Median, and Mode

An important property of the mean is that the estimator for it represents the parameter that minimizes the squared differences of the observed data from it. 

$$\hat\mu = \underset{\theta}{\operatorname{argmin}} \bigg\{\sum_{i = 1}^n(x_i - \theta)^2\bigg\}$$

We can rewrite this property as the minimum of the squared absolute values (it doesn't change anything because the squared deviations are already positive).

$$\hat\mu = \underset{\theta}{\operatorname{argmin}} \bigg\{\sum_{i = 1}^n|x_i - \theta|^2\bigg\}$$

We learn the *median* as the value of $x$ that "splits" the distribution (or population or sample) in half. An interesting property of the median is that it minimizes the *absolute* differences (to the first power) of the observed data from itself. 

$$\tilde\mu = \underset{\theta}{\operatorname{argmin}} \bigg\{\sum_{i = 1}^n|x_i - \theta|\bigg\}$$

The *mode* measures the most frequent or most likely value of $x$. An interesting property of the mode is that it minimizes the *absolute* differences of the observed data from itself *raised to the zero power*.

$$\mathring\mu = \underset{\theta}{\operatorname{argmin}} \bigg\{\sum_{i = 1}^n|x_i - \theta|^0\bigg\}$$

Notice that the distance function, $|x_i - \theta|^0$ equals $0$ if $x_i = \theta$ and $1$ if $x_i \neq \theta$, so assigning $\theta$ equal to the most frequent value minimizes the sum of these distances by setting the most values to zero instead of one. 

Summarizing, 

1. Mode: $\mathring\mu = \underset{\theta}{\operatorname{argmin}} \bigg\{\sum_{i = 1}^n|x_i - \theta|^0\bigg\}$
2. Median: $\tilde\mu = \underset{\theta}{\operatorname{argmin}} \bigg\{\sum_{i = 1}^n|x_i - \theta|^1\bigg\}$
3. Mean: $\hat\mu = \underset{\theta}{\operatorname{argmin}} \bigg\{\sum_{i = 1}^n|x_i - \theta|^2\bigg\}$

\newpage
## Variance and Standard Deviation

The second moment of *X* is $E[X^2]$; the second *central* moment of *X* around its mean is $E[(X – \mu)^2]$, also known as $V(X)$ or $\sigma_x^2$ (the first central moment, $E[X - \mu]$ is always zero). 

The standard deviation is the square root of the variance, or $\sigma_x$. 

### Discrete Random Variable

For a discrete uniform random variable from zero to ten, $Z$, calculate $Var(Z)$.

```{r discreteVz-solution}
y <- c(0:10)
py <- rep(1/length(y), length(y))
(y - c(y%*%py))^2%*%py
```

### Continuous Random Variable

For a *continuous* uniform random variable from zero to ten, $X$, calculate $Var(Z)$.

```{r continuousVx-solution}
mu <- integrate(function(x) x/10, 0, 10)$value
integrate(function(x) (x - mu)^2/10, 0, 10)
```

### Variance and Standard Deviation of a Linear Function of a Random Variable

Linear functions and variance.

1. Constants have zero variance: $V(c) = 0$ for any constant, $c$. 
2. Addition distributes within the variance: $V(X + Y) = V(X) + V(Y) + 2COV(XY)$. 
3. Coefficients factor out as squared values: $V(cX) = c^2V(X)$.
4. For any *linear* function, $f$, $V(a + bX) = b^2V(X)$ for any constants, $a$ and $b$.

In the case of standard deviation, taking square roots gives: (1) $\sigma_c = 0$; (2) $\sigma_{(x+y)} = \sqrt{\sigma_x^2 + \sigma_y^2 + 2\sigma_{sy}}$; 3. $\sigma_{cX} = c\sigma_x$; and 4. $\sigma_{(a + bX)} = b\sigma_x$. 

\newpage
## Other Moments of a Random Variable

### Skewness

-   3rd moment: $E[X^3]$
-   3rd *central* moment: $E[(X – \mu)^3]$
-   3rd *standardized* moment: $E[(\frac{X – \mu}{\sigma})^3]$

### Kurtosis

-   4th moment: $E[X^4]$
-   4th *central* moment: $E[(X – \mu)^4]$
-   4th *standardized* moment: $E[(\frac{X – \mu}{\sigma})^4]$

