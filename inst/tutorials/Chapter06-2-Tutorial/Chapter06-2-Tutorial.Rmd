---
output: slidy_presentation
runtime: shiny_prerendered
title: "Chapter 6 \n\n Multiple Regression Analysis - Further Issues"
author: "James Bang"
description: >
  This tutorial discusses interactions and other issues of model specification.
---

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(wooldridge)
library(tidyverse)
library(gganimate)
library(ggthemes)
attend <- wooldridge::attend
attend.lm1 <- lm(stndfnl ~ atndrte + priGPA + ACT + I(priGPA^2) + I(ACT^2), data = attend)
attend.lm2 <- lm(stndfnl ~ atndrte + priGPA + ACT + I(priGPA^2) + I(ACT^2) + I(atndrte*priGPA), data = attend)
attend.lm3 <- lm(stndfnl ~ atndrte + priGPA + ACT + I(priGPA^2) + I(ACT^2) + I(scale(atndrte, scale = FALSE)*scale(priGPA, scale = FALSE)), data = attend)
gradethis::gradethis_setup()
knitr::opts_chunk$set(echo = FALSE)
```

## Interactions between Continous Variables

Suppose we want to estimate the following model of (standardized) final exam scores: 

$$ Final = \beta_0 + \beta_1Attendance + \beta_2PriorGPA + \beta_3ACT + \beta_4PriorGPA^2 + \beta_5ACT^2$$
Where: 

  - $Attendance = \text{percentage of classes the student attended, } atndrte$
  - $PriorGPA = \text{student's GPA in previous classes, } priGPA$
  - $ACT = \text{student's ACT score, } ACT$

Now, suppose we suspect that the effect of attendance might itself depend on - or, be *moderated by* - prior GPA: Attendance might not matter that much for students who do perform well generally. Then, 

$$ Final = \beta_0 + \beta_1Attendance + \beta_2PriorGPA + \beta_3ACT + \beta_4PriorGPA^2 + \beta_5ACT^2 + \beta_6PriorGPA \cdot Attendance + u $$
  - Interpretation of $\hat\beta_1$: The effect of attendance on final performance *when prior GPA equals zero*. 
  - What we (probably) want to know: The effect of attendance on final performance *at the mean* (of prior GPA and other variables): $\frac{\delta{Final}}{\delta{Attendance}}\Big\rvert_{PriorGPA = \bar{x}_{PriorGPA}} = \beta_1 + \beta_6 \cdot \bar{x}_{PriorGPA}$ 
  
## Implementation

  - Execute the two regressions from the previous slide using the "attend" dataset that is pre-loaded with this tutorial. 
  - Name them $attend.lm1$ and $attend.lm2$, and summarize them using a stargazer text table. 
  - Calculate $\frac{\delta{Final}}{\delta{Attendance}}\Big\rvert_{PriorGPA = \bar{x}_{PriorGPA}}$ for each model without storing the results.

```{r attend, exercise = TRUE}


```

```{r attend-hint}
attend.lm1 <- lm(..., data = ...)
# Using the "*" operator on two variables in the formula gives the linear components as well as the interaction. 
attend.lm2 <- lm(..., data = ...)
attend.lm1$...['...']
# Always use the relevant sample for the estimation to calculate marginal effects in case the model had to omit observations due to missing data. Calculate the mean based on attend.lm2$model$variable. 
attend.lm2$...['...'] + attend.lm2$...['...'] * mean(...)
```

```{r attend-solution}
attend.lm1 <- lm(stndfnl ~ atndrte + priGPA + ACT + I(priGPA^2) + I(ACT^2), data = attend)
attend.lm2 <- lm(stndfnl ~ atndrte + priGPA + ACT + I(priGPA^2) + I(ACT^2) + I(atndrte*priGPA), data = attend)
attend.lm1$coefficients['atndrte']
attend.lm2$coefficients['atndrte'] + attend.lm2$coefficients['atndrte:priGPA'] * mean(attend.lm1$model$priGPA)
```

```{r attend-check}
grade_code()
```

## Centering Continous Interactions

A trick to make things less cumbersome is to *center* the variables in the interaction. This estimates: 
$$ Final = \beta_0 + \beta_1Attendance + \beta_2PriorGPA + \beta_3ACT + \beta_4PriorGPA^2 + \beta_5ACT^2 + \beta_6(PriorGPA - \mu_{PriorGPA}) \cdot (Attendance - \mu_{Attendance}) + u $$

Try this using the $scale()$ function (set the option $scale = FALSE$ to preserve the units of measurement). Call this $attend.lm3$.

Summarize all three $attend$ models using the stargazer command. 

```{r attend-scale, exercise = TRUE}


```

```{r attend-scale-hint}
# To only get the interaction term (not the linear components of the variables as well) we can use the "I()" operator.
attend.lm3 <- lm(..., data = attend)
stargazer(attend.lm1, attend.lm2, typeattend.lm3, type = 'text')

```

```{r attend-scale-solution}
attend.lm3 <- lm(stndfnl ~ atndrte + priGPA + ACT + I(priGPA^2) + I(ACT^2) + I(scale(atndrte, scale = FALSE)*scale(priGPA, scale = FALSE)), data = attend)
stargazer(attend.lm1, attend.lm2, typeattend.lm3, type = 'text')
```

```{r attend-scale-check}
grade_code()
```

## Comparing Specifications

Adjusted $R^2$

  - $\bar{R}^2 = 1 - \Big[\frac{SSR/(n-k-1)}{SST/(n-1)}\Big] = 1 - \Big[\frac{\hat\sigma^2}{SST/(n-1)}\Big]$
  - Useful for comparing nonnested models, e.g. switching one control for another, and alternate forms of the dependent variable. 

## Controlling for Colliders

  - Be careful adding controls! If you add a control that either *causes* or is *caused by* your treatment (and not just correlated with it), and that variable also causes the outcome, you may mask the effect of your treatment. 
  - Controlling for a variable that causes your treatment introduces *collider bias*. 

```{r, echo = FALSE}
df <- data.frame(X = rnorm(200)+1,Y=rnorm(200)+1,time="1") %>%
  mutate(C = as.integer(X+Y+rnorm(200)/2>2)) %>%
  group_by(C) %>%
  mutate(mean_X=mean(X),mean_Y=mean(Y)) %>%
  ungroup()
#Calculate correlations
before_cor <- paste("1. Start with raw data, ignoring C. Correlation between X and Y: ",round(cor(df$X,df$Y),3),sep='')
after_cor <- paste("7. Analyze what's left! Correlation between X and Y controlling for C: ",round(cor(df$X-df$mean_X,df$Y-df$mean_Y),3),sep='')
#Add step 2 in which X is demeaned, and 3 in which both X and Y are, and 4 which just changes label
dffull <- rbind(
  #Step 1: Raw data only
  df %>% mutate(mean_X=NA,mean_Y=NA,C=0,time=before_cor),
  #Step 2: Raw data only
  df %>% mutate(mean_X=NA,mean_Y=NA,time='2. Separate data by the values of C.'),
  #Step 3: Add x-lines
  df %>% mutate(mean_Y=NA,time='3. Figure out what differences in X are explained by C'),
  #Step 4: X de-meaned 
  df %>% mutate(X = X - mean_X,mean_X=0,mean_Y=NA,time="4. Remove differences in X explained by C"),
  #Step 5: Remove X lines, add Y
  df %>% mutate(X = X - mean_X,mean_X=NA,time="5. Figure out what differences in Y are explained by C"),
  #Step 6: Y de-meaned
  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=0,time="6. Remove differences in Y explained by C"),
  #Step 7: Raw demeaned data only
  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=NA,time=after_cor))
p <- ggplot(dffull,aes(y=Y,x=X,color=as.factor(C)))+geom_point()+
  geom_vline(aes(xintercept=mean_X,color=as.factor(C)), na.rm = TRUE)+
  geom_hline(aes(yintercept=mean_Y,color=as.factor(C)), na.rm = TRUE)+
  guides(color=guide_legend(title="C"))+
  scale_color_colorblind()+
  labs(title = 'Inventing a Correlation Between X and Y by Controlling for Collider C \n{next_state}', caption="Graph Code Credit: Nick Huntington-Klein, 'causalgraphs' https://github.com/NickCH-K/causalgraphs")+
  transition_states(time,transition_length=c(1,12,32,12,32,12,12),state_length=c(160,125,100,75,100,75,160),wrap=FALSE)+
  ease_aes('sine-in-out')+
  exit_fade()+enter_fade()
animate(p,nframes=200)
```

## Controlling for Post-Treatment Effects

  - Controlling for a variable that is caused by your treatment introduces post-treatment effect bias. 

```{r, echo = FALSE}
df <- data.frame(X = rnorm(200)+1,Y=rnorm(200)+1,time="1") %>%
  mutate(C = as.integer(X+Y+rnorm(200)/2>2)) %>%
  group_by(C) %>%
  mutate(mean_X=mean(X),mean_Y=mean(Y)) %>%
  ungroup()
#Calculate correlations
before_cor <- paste("1. Start with raw data, ignoring C. Correlation between X and Y: ",round(cor(df$X,df$Y),3),sep='')
after_cor <- paste("7. Analyze what's left! Correlation between X and Y controlling for C: ",round(cor(df$X-df$mean_X,df$Y-df$mean_Y),3),sep='')
#Add step 2 in which X is demeaned, and 3 in which both X and Y are, and 4 which just changes label
dffull <- rbind(
  #Step 1: Raw data only
  df %>% mutate(mean_X=NA,mean_Y=NA,C=0,time=before_cor),
  #Step 2: Raw data only
  df %>% mutate(mean_X=NA,mean_Y=NA,time='2. Separate data by the values of C.'),
  #Step 3: Add x-lines
  df %>% mutate(mean_Y=NA,time='3. Figure out what differences in X are explained by C'),
  #Step 4: X de-meaned 
  df %>% mutate(X = X - mean_X,mean_X=0,mean_Y=NA,time="4. Remove differences in X explained by C"),
  #Step 5: Remove X lines, add Y
  df %>% mutate(X = X - mean_X,mean_X=NA,time="5. Figure out what differences in Y are explained by C"),
  #Step 6: Y de-meaned
  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=0,time="6. Remove differences in Y explained by C"),
  #Step 7: Raw demeaned data only
  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=NA,time=after_cor))
p <- ggplot(dffull,aes(y=Y,x=X,color=as.factor(C)))+geom_point()+
  geom_vline(aes(xintercept=mean_X,color=as.factor(C)), na.rm = TRUE)+
  geom_hline(aes(yintercept=mean_Y,color=as.factor(C)), na.rm = TRUE)+
  guides(color=guide_legend(title="C"))+
  scale_color_colorblind()+
  labs(title = 'Inventing a Correlation Between X and Y by Controlling for Collider C \n{next_state}', caption="Graph Code Credit: Nick Huntington-Klein, 'causalgraphs' https://github.com/NickCH-K/causalgraphs")+
  transition_states(time,transition_length=c(1,12,32,12,32,12,12),state_length=c(160,125,100,75,100,75,160),wrap=FALSE)+
  ease_aes('sine-in-out')+
  exit_fade()+enter_fade()
animate(p,nframes=200)

```

## Further Issues

  - Prediction Intervals
  - Residual Analysis
  - Predicting $y$ when $log(y)$ is the dependent variable
