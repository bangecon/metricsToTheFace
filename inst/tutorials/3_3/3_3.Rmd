---
title: "Chapter 3"
subtitle: "Multiple Regression Analysis - Omitted Variable Bias and Multicollinearity"
description: "This tutorial illustrates the problems of omitted variable bias and multicollinearity."
author: 
  name: "Created by [Jim Bang](http://www.github.com/bangecon)"
  email: BangJamesT@sau.edu
  affiliation: "[St. Ambrose University](http://www.sau.edu)"
date: "`r format(Sys.Date(), '%d %b %Y')`"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
always_allow_html: yes
editor_options: 
  markdown: 
    wrap: 72
---

<style type="text/css">
h1{font-size: 24pt}
h2{font-size: 20pt}
h3{font-size: 18pt}
h4,h5,h6{font-size: 16pt}
body{font-size: 16pt}
</style>

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
tutorial_options(exercise.eval = TRUE, exercise.reveal_solution = TRUE)
gradethis::gradethis_setup()
knitr::opts_chunk$set(echo = FALSE)
```

## Omitted Variable Bias

Problem: if we misspecify our model, all bets are off! 

1. Estimated model: $y = x_1\beta_1 + e$
2. Correct model: $y = x_1\beta_1 + x_2\beta_2 + u$
3. When we omit *relevant* variables $E(Xu) = 0$ but $E(Xe)$ isn't!

The presence of omitted variable bias requires: 

1. Omitted variables, $x_2$ correlate with $y$ *and*
2. Omitted variables, $x_2$ correlate with $x_1$

### Graphical Demonstration (with Animation!)

The following graph shows how omitting a single *relevant* variable can introduce quite a bit of bias. 

```{r omittedVariableBiasDemo}
library(tidyverse)
library(gganimate)
library(gifski)
library(ggthemes)
df <- data.frame(Z = as.integer((1:200>100))) %>%
  mutate(X =  .5   + 2*Z + rnorm(200)) %>%
  mutate(Y = -.5*X + 4*Z + 1 + rnorm(200),time="1") %>%
  group_by(Z) %>%
  mutate(mean_X=mean(X),mean_Y=mean(Y)) %>%
  ungroup()
#Calculate correlations
before_cor <- paste("1. Start with raw data. Correlation between X and Y: ",round(cor(df$X,df$Y),3),sep='')
after_cor <- paste("6. Analyze what's left! Correlation between X and Y controlling for Z: ",round(cor(df$X-df$mean_X,df$Y-df$mean_Y),3),sep='')
#Add step 2 in which X is demeaned, and 3 in which both X and Y are, and 4 which just changes label
dffull <- rbind(
  #Step 1: Raw data only
  df %>% mutate(mean_X=NA,mean_Y=NA,time=before_cor),
  #Step 2: Add x-lines
  df %>% mutate(mean_Y=NA,time='2. Figure out what differences in X are explained by Z'),
  #Step 3: X de-meaned
  df %>% mutate(X = X - mean_X,mean_X=0,mean_Y=NA,time="3. Remove differences in X explained by Z"),
  #Step 4: Remove X lines, add Y
  df %>% mutate(X = X - mean_X,mean_X=NA,time="4. Figure out what differences in Y are explained by Z"),
  #Step 5: Y de-meaned
  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=0,time="5. Remove differences in Y explained by Z"),
  #Step 6: Raw demeaned data only
  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=NA,time=after_cor))
p <- ggplot(dffull,aes(y=Y,x=X,color=as.factor(Z)))+geom_point()+
  geom_vline(aes(xintercept=mean_X,color=as.factor(Z)), na.rm = TRUE)+
  geom_hline(aes(yintercept=mean_Y,color=as.factor(Z)), na.rm = TRUE)+
  guides(color=guide_legend(title="Z"))+
  scale_color_colorblind()+
  labs(title = 'The Relationship between Y and X, Controlling for a Binary Variable Z \n{next_state}', caption="Graph Code Credit: Nick Huntington-Klein, 'causalgraphs' https://github.com/NickCH-K/causalgraphs")+
  transition_states(time,transition_length=c(12,32,12,32,12,12),state_length=c(160,100,75,100,75,160),wrap=FALSE)+
  ease_aes('sine-in-out')+
  exit_fade()+enter_fade()
animate(p,nframes=200, renderer = gifski_renderer())
```

<details style="line-height:105%"><summary>Click here to see the code for creating the animated graph.</summary>

`df <- data.frame(Z = as.integer((1:200>100))) %>%`<br>
`  mutate(X =  .5   + 2*Z + rnorm(200)) %>%`<br>
`  mutate(Y = -.5*X + 4*Z + 1 + rnorm(200),time="1") %>%`<br>
`  group_by(Z) %>%`<br>
`  mutate(mean_X=mean(X),mean_Y=mean(Y)) %>%`<br>
`  ungroup()`<br>
Calculate correlations <br>
`before_cor <- paste("1. Start with raw data. Correlation between X and Y: ",round(cor(df$X,df$Y),3),sep='')`<br>
`after_cor <- paste("6. Analyze what's left! Correlation between X and Y controlling for Z: ",`<br>
`  round(cor(df$X-df$mean_X,df$Y-df$mean_Y),3),sep='')`<br>
Add step 2 in which X is demeaned, and 3 in which both X and Y are, and 4 which just changes label <br>
`dffull <- rbind(`<br>
  Step 1: Raw data only <br>
`  df %>% mutate(mean_X=NA,mean_Y=NA,time=before_cor),`<br>
  Step 2: Add x-lines <br>
`  df %>% mutate(mean_Y=NA,time='2. Figure out what differences in X are explained by Z'),`<br>
  Step 3: X de-meaned <br>
`  df %>% mutate(X = X - mean_X,mean_X=0,mean_Y=NA,time="3. Remove differences in X explained by Z"),`<br>
  Step 4: Remove X lines, add Y <br>
`  df %>% mutate(X = X - mean_X,mean_X=NA,time="4. Figure out what differences in Y are explained by Z"),`<br>
  Step 5: Y de-meaned <br>
`  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=0,`<br>
`  time="5. Remove differences in Y explained by Z"),`<br>
  Step 6: Raw demeaned data only<br>
`  df %>% mutate(X = X - mean_X,Y = Y - mean_Y,mean_X=NA,mean_Y=NA,time=after_cor))`<br>
`p <- ggplot(dffull,aes(y=Y,x=X,color=as.factor(Z)))+geom_point()+`<br>
`  geom_vline(aes(xintercept=mean_X,color=as.factor(Z)), na.rm = TRUE)+`<br>
`  geom_hline(aes(yintercept=mean_Y,color=as.factor(Z)), na.rm = TRUE)+`<br>
`  guides(color=guide_legend(title="Z"))+`<br>
`  scale_color_colorblind()+`<br>
`  labs(title = 'The Relationship between Y and X, Controlling for a Binary Variable Z \n{next_state}',`<br>
`    caption="Graph Code Credit: Nick Huntington-Klein, 'causalgraphs'`<br>
`    https://github.com/NickCH-K/causalgraphs")+`<br>
`  transition_states(time,transition_length=c(12,32,12,32,12,12),state_length=c(160,100,75,100,75,160), `<br>
`    wrap=FALSE)+`<br>
`  ease_aes('sine-in-out')+`<br>
`  exit_fade()+enter_fade()`<br>
`animate(p,nframes=200, renderer = gifski_renderer())`<br>
</details>

### Mathematical Derivation

$$E(\tilde\beta_1) = E[(X_1'X_1)^{-1}X_1'(X_1\beta_1 + X_2\beta_2 + u)]$$  

$$ E(\tilde\beta_1) = \beta_1 + \beta_2E[(X_1'X_1)^{-1}X_1'X_2] $$  

We can rewrite this as: 

$$ E(\hat\beta_1) = \beta_1 + \beta_2\delta$$  

where $\delta$ is the coefficient from regressing $X_2$ on $X_1$, $X_2 = X_1\delta + e$. 

### Irrelevant Variables

1. Generate a random, independent variable, `sunspots`, which takes values from a Poisson distribution with parameter $\lambda = 10$ , as a new column in `wage1`. 
2. Regress `wage` on `educ`, `exper`, and `sunspots`.
3. Compare this to the regression of `wage` on `educ` and `exper` (excluding `sunspots`) by creating a text table in `stargazer()`.

```{r irrelevant, exercise = TRUE}

```

```{r irrelevant-hint-1}
# Remember that the `rpois(n, lambda)` function generates random samples from a Poisson distribution. 
# Poisson distributions are a commonly-used theoretical approximation of random events that describe the number of times something occurs in a given time interval. 
# A Poisson distribution has a variance equal to its mean, lambda. 
```

```{r irrelevant-hint-2}
# This is science! It should be reproducible! Set a seed (`set.seed(8675309)`) before randomizing!
```

```{r irrelevant-hint-3}
# The exponential distribution is complementary to the Poisson: if a Poisson distribution describes the number of times something randomly occurs in a given time interval, then the time between occurrences will follow an exponential distribution with mean 1/lambda and variance 1/lambda^2.
```

```{r irrelevant-solution}
set.seed(8675309)
library(stargazer)
wage1 <- wooldridge::wage1
wage.lm2 <- lm(wage ~ educ + exper, data = wage1)
wage1$sunspots <- rpois(length(wage1$wage), lambda = 10)
wage.lm3 <- lm(wage ~ educ + exper + sunspots, data = wage1)
stargazer(wage.lm2, wage.lm3, type = 'text')
```

```{r irrelevant-check}
grade_this({
  if (identical(.result, .solution)) {
    pass(random_praise())
  }
  fail(random_encouragement())
})
```

### A Caveat about Omitted Variables

```{r linearRegression, echo=FALSE}
question("Should tests for gender wage discrimination control for occupational choice?",
    answer("Yes", correct = TRUE, message = "You're right IF differences in occupation are NOT caused by gender discrimination."),
    answer("No", correct = TRUE, message = "You're right IF differences in occupation ARE caused by gender discrimination."),
    allow_retry = TRUE
  )
```

Sometimes it's helpful to draw the hypothesized pattern of causality. There are some neat tools for this in the `ggdag` package. 

```{r ggdagDemo, echo = TRUE}
library(ggdag)
wageGapDag  <- dagify(Wage ~ Educ + Gender + Occ,
  Occ ~ Educ + Gender)
ggdag(wageGapDag) 
ggdag_collider(wageGapDag)
```

## Multicollinearity

### Perfect Multicollinearity

When a group of variables is perfectly collinear (*linearly* correlated), we cannot invert $X'X$. It becomes akin to dividing by zero. Example: White/Nonwhite. 

1. Using the wage1 data create the variable `white` equal to one minus `nonwhite`. 
2. Regress `wage` on `educ`, `exper`, `white` and `nonwhite`. 
3. Summarize the results with `summary()`. 

```{r multicollinear, exercise = TRUE}


```

```{r multicollinear-setup}
wage1 <- wooldridge::wage1
```

```{r multicollinear-hint}

```

```{r multicollinear-solution}
wage1$white <- 1 - wage1$nonwhite
wage.lm4 <- lm(wage ~ educ + exper + white + nonwhite, data = wage1)
summary(wage.lm4)
```

```{r multicollinear-check}
grade_this({
  if (identical(.result$call, .solution$call)) {
    pass(random_praise())
  }
  fail(random_encouragement())
})
```

### Perfect Multicollinearity without a Constant

Suppose I want to know the absolute magnitude of the intercept for each group (white/nonwhite).

1. One way I could do this is by adding. The baseline for the omitted group (here, `nonwhite`) is the regular intercept. 
2. Another way I could do this is to regress the model with both variables, excluding the intercept. 

```{r multicollinearNoConstant, exercise = TRUE}

```

```{r multicollinearNoConstant-setup}
wage1 <- wooldridge::wage1
wage1$white <- 1 - wage1$nonwhite
wage.lm4 <- lm(wage ~ educ + exper + white + nonwhite, data = wage1)
```

```{r multicollinearNoConstant-hint}

```

```{r multicollinearNoConstant-solution}
wage.lm5 <- lm(wage ~ educ + exper + white + nonwhite - 1, data = wage1)
summary(wage.lm5)
```

```{r multicollinearNoConstant-check}
grade_this({
  if (identical(.result$call, .solution$call)) {
    pass(random_praise())
  }
  fail(random_encouragement())
})
```

We would rarely want to do it this way. If we are interested in how some categorical variable affects the outcome as a research hypothesis, we typically would only want to know how the outcome for the "treated" group *differs* from the "control" group. If we don't care how that variable affects the outcome (i.e. it is "just a control variable"), then there is no reason to go to the trouble. 

### Imperfect Multicollinearity

Variance Formula - Scalar Form

Simple Regression Model

$$ Var(\hat\beta_j) = \frac{s^2}{SST_x} $$

Multiple Regression Model

$$\begin{align} \hat{Var}(\hat\beta_j) &= \frac{\hat{\sigma}_u^2}{SST_j(1-R_j^2)} \\
&= \frac{\hat{\sigma}_u^2}{(n-1)\hat{Var}(x_j)} \cdot \frac{1}{(1-R_j^2)} \\
&= \tilde{s}_{\hat{\beta}_j}^2 \cdot VIF \end{align}$$

Effect of Multicollinearity
  1. $SST_j = \sum_{i=1}^n{(x_{ij} - \bar{x}_j)^2}$
  2. $R_j^2$ is the $R^2$ obtained from regressing $x_j$ on all other $x'$s and a constant. 
  3. The more correlated $x_j$ is with the other $x'$s, the more inflated the variance becomes.

Bias-variance trade-off.
