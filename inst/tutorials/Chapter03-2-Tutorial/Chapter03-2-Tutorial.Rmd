---
output: slidy_presentation
runtime: shiny_prerendered
title: "Chapter 3 \n\n Multiple Regression Analysis - Gauss-Markov Theorem"
author: "James Bang"
description: >
  This tutorial introduces linear estimation with multiple variables.
---

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(wooldridge)
wage1 <- wooldridge::wage1
gradethis::gradethis_setup()
knitr::opts_chunk$set(echo = FALSE)
```

## Gauss-Markov Theorem

Under the assumptions of the classical regression model, the OLS estimates of $\hat\beta$ are \textcolor{blue}{BLUE}.

- Best: Efficient, or minimum-variance (compared to estimates that fit the other criteria).
- Linear: Linear in the parameters.
- Unbiased: $E(\hat\beta_{OLS}) = \beta$. 
- Estimator. 

Since linearity is trivially true by OLS assumption #1, we will prove bestness and unbiasedness, starting with unbiasedness (OLS needs to be unbiased before it can be the *best* unbiased!)

## OLS in Matrix Form

The proofs are a bit easier if we express the estimator in matrix form. In matrix form, OLS solves 
$$ min_\hat\beta (X\hat\beta\ - y)'(X\hat\beta - y) $$
$$ min_\hat\beta (\hat\beta X'X \hat\beta - 2\hat\beta'X'y - y'y)  $$
The first order condition solves
$$ 2X'X\hat\beta - 2X'y = 0 $$
The solution to which is
$$ \hat\beta = (X'X)^{-1}X'y $$
The first part (the part with the inverse) is just the sums of squares of the X variables with themselves (the diagonal) and one another (off-diagonal). The rest is the transpose of the X matrix cross-multiplied with the vector of y values. 

## OLS in Matrix Form

## Exercise

Using the $wage1$ dataset calculate the regression coefficients and their standard errors for the regression of $wage$ on education ($educ$) and experience ($exper$). 

Note you will need to define n (the number of rows in the data), y, X, k, and (after calculating $\hat\beta$) $uhat$. 

```{r olsMatrix, exercise=TRUE, exercise.reveal_solution = TRUE}

```

```{r olsMatrix-solution}
n <- nrow(wage1)
y <- wage1$wage
X <- cbind(1, wage1$educ, wage1$exper)
k <- ncol(X) - 1
bhat <- solve( t(X)%*%X ) %*% t(X)%*%y
uhat <- y - X %*% bhat
sigsqhat <- as.numeric( t(uhat) %*% uhat / (n-k-1) )
SER <- sqrt(sigsqhat)
Vbetahat <- sigsqhat * solve( t(X)%*%X )
se <- sqrt( diag(Vbetahat) )
```

```{r olsMatrix-hint}
n <- nrow(...)
y <- ...
X <- cbind(..., ..., ...)
k <- ncol(...) - ...
# To transpose a matrix, use the function t(); to calculate the matrix product use %*%; and to invert a matrix use the solve() function.
bhat <- solve(...%*%... ) %*% ...%*%...
uhat <- y - X %*% bhat
# when you calculate sigsqhat define the results as a scalar number using as.numeric().
sigsqhat <- as.numeric( ... %*% ... / (...) )
SER <- sqrt(...)
Vbetahat <- ... * solve( ...%*%... )
# The variances of the betas are on the diagonal of the variance matrix; the standard errors are their square root. 
se <- sqrt( ... )
```

```{r olsMatrix-check}
grade_code()
```

## Unbiased-ness: Expected Value of $\hat\beta_{OLS}$ 

<html>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I find this is particularly difficult in explaining two-step estimators, such as feasible GLS or a control function method. The infeasible estimator could (although need not) be unbiased. The feasible version rarely is.<a href="https://twitter.com/hashtag/metricstotheface?src=hash&amp;ref_src=twsrc%5Etfw">#metricstotheface</a> <a href="https://t.co/oJfmwvS6if">pic.twitter.com/oJfmwvS6if</a></p>&mdash; Jeffrey Wooldridge (@jmwooldridge) <a href="https://twitter.com/jmwooldridge/status/1378804239309365258?ref_src=twsrc%5Etfw">April 4, 2021</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</html>

$$ E(\hat\beta) = E[(X'X)^{-1}X'y] $$
Substituting $X\hat\beta - u$ for $y$, 
$$ E(\hat\beta) = E[(X'X)^{-1}X'(X\beta+u)]$$  
Distributing $ (X'X)^{-1}X' $ through the parentheses, 
$$ E(\hat\beta) = E[(X'X)^{-1}X'X\beta+(X'X)^{-1}X'u)]$$ 
In the first term, note that the anything times its multiplicative inverse (one over something, but in this case the matrix inverse) cancels out (becomes an identity matrix, or just 1 for a scalar). 
$$ E(\hat\beta) = \beta + E[(X'X)^{-1}X'u] $$
In the second term, note that zero conditional expectation of the errors implies that X and u are uncorrelated, or $E(X'u) - E(X)E(u) = 0$. Since $E(u) = 0$, this implies $E(X'u) = 0$.

## Variance of $\hat\beta$

$$ Var(\hat\beta) = Var[(X'X)^{-1}X'y] $$
Let $ Z = (X'X)^{-1}X'$ and substitute $y = X\beta + u$, so we have 
$$ Var(\hat\beta) = Var[Z(X\beta+u)] $$
$$ Var(\hat\beta) = Var(ZX\beta+Zu) $$

Since $\beta$ is just a parameter value, the first term is simply a constant. 
$$ Var(\hat\beta) = Var(Zu) $$
Factoring out Z (which we have to square by the rules of the variance of a linear comgination) we have
$$ Var(\hat\beta) = Z'ZVar(u) = Z'Z\sigma^2 $$
$$ Var(\hat\beta) = X(X'X)^{-1}(X'X)^{-1}X'\sigma^2 = (X'X)^{-1}\sigma^2$$

##Comparison to other linear, unbiased estimators

$$ Var(\hat\beta) = Var(Zy) = Z'Z\sigma^2$$
Let $C = Z + D$ so that it gives a different linear estimate, $\tilde\beta = Cy$. Note that for $\tilde\beta$ to be unbiased, DX must equal 0. Then through a similar set of steps as the previous slide, 
$$ Var(\tilde\beta) = Var(Cy) = C'C\sigma^2 $$
Reverting to $Z + D$ for $C$, 
$$ Var(\tilde\beta) = (Z+D)'(Z+D)\sigma^2 $$
$$ Var(\tilde\beta) = (Z'Z+2Z'D+D'D)\sigma^2 $$
Since $\tilde\beta$ is still unbiased $Z'D = (X'X)^-1XD = 0$, so
$$ Var(\tilde\beta) = (Z'Z+D'D)\sigma^2 = Var(\hat\beta) + D'D\sigma^2 $$
Since $D'D$ is like squaring the matrix, D, it is *positive definite*, which means that the any linear adjustment to the OLS estimator that is also unbiased will have higher variance.

