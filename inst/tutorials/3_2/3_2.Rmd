---
title: "Chapter 3"
subtitle: "Multiple Regression Analysis - Gauss-Markov Theorem"
description: "This tutorial illustrates the Gauss-Markov Theorem."
author: 
  name: "Created by [Jim Bang](http://www.github.com/bangecon)"
  email: BangJamesT@sau.edu
  affiliation: "[St. Ambrose University](http://www.sau.edu)"
date: "`r format(Sys.Date(), '%d %b %Y')`"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
runtime: shiny_prerendered
always_allow_html: yes
editor_options: 
  markdown: 
    wrap: 72
---

<style type="text/css">
h1{font-size: 24pt}
h2{font-size: 20pt}
h3{font-size: 18pt}
h4,h5,h6{font-size: 16pt}
body{font-size: 16pt}
</style>

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
tutorial_options(exercise.eval = TRUE, exercise.reveal_solution = TRUE)
gradethis::gradethis_setup()
knitr::opts_chunk$set(echo = FALSE)
```

## OLS in Matrix Form

The proofs in this tutorial are a bit easier if we express the estimator in matrix form. In matrix form, OLS solves 
$$ min_\hat\beta (X\hat\beta\ - y)'(X\hat\beta - y) $$
$$ min_\hat\beta (\hat\beta' X'X \hat\beta - 2\hat\beta'X'y - y'y)  $$
The first order condition solves
$$ 2X'X\hat\beta - 2X'y = 0 $$
The solution to which is
$$ \hat\beta = (X'X)^{-1}X'y $$
The first part (the part with the inverse) is just the sums of squares of the $X$ variables with themselves (the diagonal) and one another (off-diagonal). The rest is the transpose of the $X$ matrix cross-multiplied with the vector of $y$ values. 

### Exercise

Using the `wage1` dataset calculate the regression coefficients and their standard errors for the regression of `wage` on education (`educ`) and experience (`exper`). 

Note you will probably want to define `n`, `y`, `X`, and `k`. 

```{r olsMatrix, exercise=TRUE, exercise.reveal_solution = TRUE}

```

```{r olsMatrix-hint-1}
# The sample size, `n` is the number of complete cases in the data for the variables in the model. 
# `complete.cases(df)` gives the number of complete cases in the data frame. 
# `df[complete.cases(df), ] retains only the rows that are complete. 
```

```{r olsMatrix-hint-2}
# To transpose a matrix, use the function t(). 
# To calculate the matrix product use %*%. 
# To invert a matrix use the solve() function.
```

```{r olsMatrix-solution}
wage1 <- wooldridge::wage1
n <- nrow(wage1)
y <- wage1$wage
X <- cbind(1, wage1$educ, wage1$exper)
k <- ncol(X) - 1
bhat <- solve( t(X)%*%X ) %*% t(X)%*%y
```

```{r olsMatrix-check}
grade_this({
  if (identical(.result, .solution)) {
    pass(random_praise())
  }
  fail(random_encouragement())
})
```

## Gauss-Markov Theorem

Under the assumptions of the classical regression model, the OLS estimates of $\hat\beta$ are \textcolor{blue}{BLUE}.

- Best: Efficient, or minimum-variance (compared to estimates that fit the other criteria).
- Linear: Linear in the parameters.
- Unbiased: $E(\hat\beta_{OLS}) = \beta$. 
- Estimator. 

Since linearity is trivially true by OLS assumption #1, we will prove bestness and unbiasedness, starting with unbiasedness (OLS needs to be unbiased before it can be the *best* unbiased!)

But wait! Actually, [Hansen (2022)](https://onlinelibrary.wiley.com/doi/10.3982/ECTA19255#:~:text=The%20Gauss%E2%80%93Markov%20theorem%20states,is%20the%20least%20squares%20estimator.) shows that under the classical assumptions (without assuming normality in the errors as is assumed in your book) OLS is even best among *nonlinear* estimators (i.e. BUE or MVUE)! 

### Unbiased-ness 

$$E(\hat\beta) = E[(X'X)^{-1}X'y]$$
Substituting $X\hat\beta - u$ for $y$, 

$$ E(\hat\beta) = E[(X'X)^{-1}X'(X\beta+u)]$$  

Distributing $(X'X)^{-1}X'$ through the parentheses, 

$$E(\hat\beta) = E[(X'X)^{-1}X'X\beta+(X'X)^{-1}X'u)]$$

In the first term, note that the anything times its multiplicative inverse (one over something, but in this case the matrix inverse) cancels out (becomes an identity matrix, or just 1 for a scalar). 

$$E(\hat\beta) = \beta + E[(X'X)^{-1}X'u]$$

In the second term, note that zero conditional expectation of the errors implies that X and u are uncorrelated, or $E(X'u) - E(X)E(u) = 0$. Since $E(u) = 0$, this implies $E(X'u) = 0$.

### Efficiency Relative to Other Linear Estimators

Efficiency means that the estimator is the lowest-variance estimator among all estimators fitting a certain criteria (such as being linear and unbiased).

$$Var(\hat\beta) = Var[(X'X)^{-1}X'y]$$

Calculate the vector, $\hat{\sigma}_{\hat{\beta}}$.

```{r sebhat, exercise = TRUE}

```


```{r sebhat-setup}
wage1 <- wooldridge::wage1
n <- nrow(wage1)
y <- wage1$wage
X <- cbind(1, wage1$educ, wage1$exper)
k <- ncol(X) - 1
bhat <- solve( t(X)%*%X ) %*% t(X)%*%y
```

```{r sebhat-hint1}
# The setup stores the sample size as `n`; 
  # the dependent variable as `y`; 
  # the matrix of independent variables (including the constant) as `X`; 
  # the number of slope parameters (excluding the constant, following the book) as `k`; 
  # the OLS coefficients (including the constant) as `bhat`.
```

```{r sebhat-hint-2}
# To transpose a matrix, use the function t(); to calculate the matrix product use %*%; and to invert a matrix use the solve() function.
# when you calculate sigsqhat define the results as a scalar number using as.numeric().
# The variances of the betas are on the diagonal of the variance matrix; the standard errors are their square root. 
```

```{r sebhat-solution}
uhat <- y - X %*% bhat
sigsqhat <- as.numeric( t(uhat) %*% uhat / (n-k-1) )
SER <- sqrt(sigsqhat)
Vbetahat <- sigsqhat * solve( t(X)%*%X )
se <- sqrt( diag(Vbetahat) )
cbind(bhat, se)
```

```{r sebhat-check}
grade_this({
  if (identical(.result, .solution)) {
    pass(random_praise())
  }
  fail(random_encouragement())
})
```

To show that this estimate of $\hat{\sigma}_{\hat{\beta}}$ is efficient, we can "simplify" the matrix formula somewhat and check to see if there is an alternative variance that is less. 

Let $Z = (X'X)^{-1}X'$ and substitute $y = X\beta + u$, so we have 

$$Var(\hat\beta) = Var[Z(X\beta+u)]$$

$$Var(\hat\beta) = Var(ZX\beta+Zu)$$

Since $\beta$ is a constant (albeit unknown), 

$$Var(\hat\beta) = Var(Zu)$$
Factoring out Z (which we have to "square" (in matrix terms) by the rules of the variance of a linear combination) we have

$$Var(\hat\beta) = Z'ZVar(u) = Z'Z\sigma^2$$

$$Var(\hat\beta) = X(X'X)^{-1}(X'X)^{-1}X'\sigma^2 = (X'X)^{-1}\sigma^2$$

#### Comparison to other linear, unbiased estimators

$$ Var(\hat\beta) = Var(Zy) = Z'Z\sigma^2$$

Let $C = Z + D$ so that it gives a different linear estimate, $\tilde\beta = Cy$. For $\tilde\beta$ to be unbiased, DX must equal 0. Then through a similar set of steps as the previous slide, 

$$ Var(\tilde\beta) = Var(Cy) = C'C\sigma^2$$

Reverting to $Z + D$ for $C$, 

$$ Var(\tilde\beta) = (Z+D)'(Z+D)\sigma^2 $$

$$ Var(\tilde\beta) = (Z'Z+2Z'D+D'D)\sigma^2 $$
Since $\tilde\beta$ is still unbiased $Z'D = (X'X)^-1XD = 0$, so

$$ Var(\tilde\beta) = (Z'Z+D'D)\sigma^2 = Var(\hat\beta) + D'D\sigma^2 $$

Since $D'D$ is like squaring the matrix, D, it is *positive definite*, which means that the any linear adjustment to the OLS estimator that is also unbiased will have higher variance.
