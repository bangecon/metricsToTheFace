---
output: 
  learnr::tutorial:
    progressive: true
runtime: shiny_prerendered
title: "Chapter 2 \n\n The Simple Regression Model"
author: "James Bang"
description: >
  This tutorial discusses interpretation of the linear coefficients and the properties of the OLS estimator.
---

<style type="text/css">
h1{font-size: 24pt}
h2{font-size: 20pt}
h3{font-size: 18pt}
h4,h5,h6{font-size: 16pt}
body{font-size: 16pt}
</style>

<script language="JavaScript">
$(function() {
   var editor;
   $('.ace_editor').each(function( index ) {
     editor = ace.edit(this);
     editor.setFontSize("16px");
   });
})
</script>

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(wooldridge)
wage1 <- wooldridge::wage1
lwage.lm1 <- lm(log(wage) ~ educ, data = wage1)
gradethis::gradethis_setup()
knitr::opts_chunk$set(echo = FALSE)
```

## Nonlinearities

```{r linearRegression, echo=FALSE}
question("What do we mean by 'linear regression?'",
    answer("that the population regression function is linear in the independent variable(s)", message = random_encouragement()),
    answer("that the true relationship between the variables must be linear", message = random_encouragement()),
    answer("that the population regression function is linear in the parameters", correct = TRUE,  message = random_praise()),
    answer("that the regression line minimizes the sum of squared residuals", message = random_encouragement()),
    allow_retry = TRUE
  )
```

### Wages and Education

- Estimate a linear model for the log of wages on the level of education and call it lwage.lm1. 
- Summarize the output

```{r logLinear, exercise = TRUE}


```

```{r logLinear-hint}
# You may use mathematical operators on variables within your formula() argument, e.g. log(wage).
```

```{r logLinear-solution}
lwage.lm1 <- lm(log(wage) ~ educ, data = wage1)
summary(lwage.lm1)
```

```{r logLinear-check}
grade_this({if (!inherits(.result, c("summary.lm"))) {
    fail("Your class of your answer should be a summary of an lm object.")}
  if (.result$coefficients[2] != 0.08274437) { 
    fail("The coefficient on education should be about 0.083.") }
  pass()
})
```

### Interpreting Regression Coefficients

```{r betaInterpretationLinear, echo=FALSE}
question("In the regression, $wage = \\beta_0 + \\beta_1educ + u$, what is the economic interpretation of $\\beta_1$?",
    answer("that a one-year increase in education leads to a $\\beta_1$ dollar increase in hourly wage on average", correct = TRUE, message = random_praise()),
    answer("that a one-year increase in education leads to a $\\beta_1$ percent increase in hourly wage on average", message = random_encouragement()),
    answer("that a one percent increase in education leads to a $\\beta_1$ percent increase in hourly wage on average", message = random_encouragement()),
    answer("that a one-year increase in education leads to a $\\beta_1$ dollar increase in hourly wage always", message = random_encouragement()),
    random_answer_order = TRUE,
    allow_retry = TRUE
  )
```

```{r betaInterpretationLogLinear, echo=FALSE}
question("In the regression, $\\log(wage) = \\beta_0 + \\beta_1educ + u$, what is the economic interpretation of $\\beta_1$?",
    answer("that a one-year increase in education leads to a $\\beta_1$ dollar increase in hourly wage on average", message = random_praise()),
    answer("that a one-year increase in education leads to a $\\beta_1$ percent increase in hourly wage on average", correct = TRUE, message = random_encouragement()),
    answer("that a one percent increase in education leads to a $\\beta_1$ percent increase in hourly wage on average", message = random_encouragement()),
    answer("the log-linear model is better than the linear model", message = random_encouragement()),
    random_answer_order = TRUE,
    allow_retry = TRUE
  )
```

## Expected Values and Variances of OLS Estimators

Recall in the practice for Appendix C we simulated 1000 resamples of a simple OLS regression for sample sizes from 1 to 100. Run the following code to view the density plots for this simulation with vertical lines colored corresponding to the density they describe and textured differently for visibility. 

```{r E_betaHat, exercise = TRUE}
set.seed(8675309)
X <- NULL
u <- NULL
Y <- NULL
b1 <- matrix(NA, nrow = 1000, ncol = 100)
for (i in 1:100) {
  for (j in 1:1000) {
    X <- rexp(n = i, rate = 1)
    u <- rnorm(n = i, mean = 0, sd = 1)
    Y = 2 - 0.5 * X + u
    b1[j, i] = lm(Y ~ X)$coefficients[2]
  }
}
plot(
  density(b1[, 5]),
  xlim = c(-2, 1),
  ylim = c(0, 4),
  col = 'red',
  main = "Sampling Distributions of the OLS Estimator",
  xlab = expression(hat(beta)[1])
)
lines(density(b1[, 10]), col = 'gold')
lines(density(b1[, 30]), col = 'green')
lines(density(b1[, 100]), col = 'blue')
abline(v = mean(b1[, 5]), col = 'red', lty = 'dotted')
abline(v = mean(b1[, 10]), col = 'gold', lty = 'solid')
abline(v = mean(b1[, 30]), col = 'green', lty = 'dashed')
abline(v = mean(b1[, 100]), col = 'blue', lty = 'dotted')
```

### Unbiasedness of the OLS Estimator

Assumptions: 

1. Linearity in Parameters
2. Random Sampling: $(X_i, Y_i)$ are independently and identically distributed
3. Sample Variation: $x_i$s vary - rules out perfect collinearity with constant
4. Zero Conditional Mean of $u$. $$E(u|x) = 0$$

$$\hat{\beta_1} = \frac{\sum_{i=1}^n (x_i - \bar{x})y_i}{\sum_{i=1}^n (x_i - \bar{x})^2}$$

$$E[\hat{\beta_1}] = E\bigg[\frac{\sum_{i=1}^n (x_i - \bar{x})(\beta_0+\beta_1x_i + u_i)}{\sum_{i=1}^n (x_i - \bar{x})^2}\bigg]$$
$$ = E\bigg[\frac{\beta_0\sum_{i=1}^n (x_i - \bar{x})}{\sum_{i=1}^n (x_i - \bar{x})^2} + \frac{\beta_1\sum_{i=1}^n (x_i - \bar{x})x_i}{\sum_{i=1}^n (x_i - \bar{x})^2} + \frac{\sum_{i=1}^n (x_i - \bar{x})u_i}{\sum_{i=1}^n (x_i - \bar{x})^2}) \bigg]$$
The following properties give us the result: 
1. $\sum_{i=1}^n (x_i - \bar{x}) = \sum_{i=1}^n x_i - n\sum_{i=1}^n \frac{x_i}{n}) =  0 \Rightarrow E\bigg[\frac{\beta_0\sum_{i=1}^n (x_i - \bar{x})}{\sum_{i=1}^n (x_i - \bar{x})^2}\bigg] = 0$
2. $\sum_{i=1}^n (x_i - \bar{x})x_i = \sum_{i=1}^n x_i^2 - n\bar{x}^2 = \sum_{i=1}^n (x_i - \bar{x})^2 \Rightarrow \frac{\beta_1\sum_{i=1}^n (x_i - \bar{x})x_i}{\sum_{i=1}^n (x_i - \bar{x})^2} = \beta_1$
3. $E\bigg[\sum_{i=1}^n (x_i - \bar{x})u_i \bigg] = 0$ by the assumption that the expectation of the errors conditional on $x$ equals zero. 

### Variance of the OLS Estimator

Additional Assumption: 

- Homoskedasticity: $u_i{'s}$ have constant variance regardless of the value of $x$. $$Var(u|x) = 0$$

Standard Error of the Regression: $\hat{\sigma} = \sqrt{ \frac{1}{n-2}\sum_{i=1}^n{u_i^2}}$ - the standard deviation of the residuals

$$ \hat{\sigma}_{\hat{\beta}_1}^2 = \frac{\hat{\sigma}^2}{\sum_{i=1}^n(x_i-\bar{x})^2} = \frac{\hat{\sigma}^2}{(n-1)\hat{\sigma}_x^2}$$

$$ \hat{\sigma}_{\hat{\beta}_0}^2 = \frac{\hat{\sigma}^2\sum_{i=1}^n{x_i^2}}{n\sum_{i=1}^n(x_i-\bar{x})^2}= \frac{\hat{\sigma}^2}{(n-1)\hat{\sigma_x}^2} \cdot\frac{\sum_{i=1}^n{x_i^2}}{n} $$

### Sampling Distribution of $\hat{\beta}_1$ & $\hat{\beta}_0$

By the Central Limit Theorem, for sufficiently large $n$, 

$$ \hat{\beta}_1 \sim N(\beta_1, \frac{\sigma_{\hat{\beta_1}}^2}{n})$$

$$ \hat{\beta}_0 \sim N(\beta_0, \frac{\sigma_{\hat{\beta_0}}^2}{n})$$
