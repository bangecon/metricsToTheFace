---
output: pdf_document
classoption: landscape
title: "Chapter 3 \n\n Multiple Regression Analysis - Estimation"
author: "Jim Bang"
description: "This tutorial introduces linear estimation with multiple variables."
---

```{r setup, include=FALSE}
library(stargazer)
library(xfun)
library(htmltools)
library(wooldridge)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
wage1 <- wooldridge::wage1
wage.lm1 <- lm(wage ~ educ, data = wage1)
wage.lm2 <- lm(wage ~ educ + exper, data = wage1)
```

## Multiple Regression

### Wages and Education AND MORE!

- Using the wage1 data, regress wages on education (with an intercept). 

Name this wage.lm1

- Using the wage1 data, regress wages on education and experience (with an intercept). 
- Name this wage.lm2
- Summarize each regression object using "summary()"

```{r multipleLM-solution}
wage.lm1 <- lm(wage ~ educ, data = wage1)
wage.lm2 <- lm(wage ~ educ + exper, data = wage1)
summary(wage.lm1)
summary(wage.lm2)
```

\newpage 
### Table Output for Regressions

- Display a *stargazer* object of each of the previous regressions using the *stargazer* function in the *stargazer* package. 
- Display the output without assigning it to a named object 
- Use a *text* output type. (In general, it's more useful to use $type = \text{'html'}$ and $out = \text{'filename.html'}$)

```{r gtLM-solution}
stargazer(wage.lm1, wage.lm2, type = 'text')
```

\newpage
### Raw Stargazer HTML Output 

```{r}
stargazer(wage.lm1, wage.lm2, type = 'html')
```

\newpage 
### Formatted Stargazer HTML Output 

You can directly insert a `stargazer` table in an `html` Rmarkdown document. One is to include the command that generates the `html` in an `R` chunk (with the option `results='asis'`) that itself is inside an `html` code chunk.  

```{r, eval=FALSE}
stargazer(
  wage.lm1,
  wage.lm2,
  title = "Education and Wages",
  type = "html",
  float = TRUE,
  no.space = TRUE,
  header = FALSE,
  covariate.labels = c("Education", "Experience")
)
```

### Embedding Stargazer Output as HTML

Alternately, you might want to run your code from a `R` source file, save your pretty tables as html files, and include the content of those files in an `html` document. 

One way to achieve this is to insert a code chunk with the `htmltools::includeHTML()` function: 

```{r eval = FALSE}
includeHTML('output/wage.html')
```

Equivalently, you could do the same thing using the `xfun::file_string()` function: 

```{r eval = FALSE}
file_string('output/wage.html')
```

Since this tutorial knits as an `html` document I needed to use some extra tricks. More commonly, you would knit your document as a `pdf`, and in this case you would want to leave out the `html` wrap, and set `type = "latex"`. 

```{r results='asis'}
stargazer(
  wage.lm1,
  wage.lm2,
  title = "Education and Wages",
  type = "latex",
  float = TRUE,
  no.space = TRUE,
  header = FALSE,
  covariate.labels = c("Education", "Experience")
)
```

\newpage 
## Implementing Regression

### Assumptions of the Classical Regression Model

1. Linear in the parameters
2. Random Sampling: $(X_i, Y_i)$ are independently and identically distributed
3. No Perfect Multicollinearity
4. $E[u|x] = 0$
5. Homoskedasticity: $u_i{'s}$ have constant variance regardless of the value of $x$
6. Large Outliers are Unlikely: $X$ and $Y$ have finite fourth moments
$$E(X^4)< \infty$$
$$E(Y^4)< \infty$$

### Deriving the OLS Estimator

One variable

$$ \min_{\hat\beta_0,\hat\beta_1} \sum_{i=1}^n{(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1})^2} $$

Two variables

$$ \min_{\hat\beta_0,\hat\beta_1,\hat\beta_2} \sum_{i=1}^n{(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-\hat{\beta}_2x_{i2})^2} $$

$k$ variables

$$ \min_{\hat\beta_0,\hat\beta_1,...,\hat\beta_k} \sum_{i=1}^n{(y_i-\hat{\beta}_0-\hat{\beta}_1x_{i1}-...-\hat{\beta}_kx_{ik})^2} $$

Taking the derivative with respect to each of the $\hat{\beta}$'s separately yields a system of equations equal to the number of $\hat{\beta}$'s {$k + 1$). 

### Interpretating the Coefficients

Predicted values of y:
$$\hat{wage}=\hat{\beta}_0+\hat{\beta}_1educ+\hat{\beta}_2exper$$
Changes in $\hat{y}$:
$$ \Delta \hat{wage}=\hat{\beta}_0+\hat{\beta}_1 \Delta educ + \hat{\beta}_2 \Delta exper$$
*Ceteris paribus* the predicted change in $y$ in response to an observed change in $x$ is:
$$ \Delta \hat{wage}=0+\hat{\beta}_1 \Delta educ+\hat{\beta}_2 (0)$$
$$ \Delta \hat{wage}=\hat{\beta}_1 \Delta educ $$
The predicted change in wage is $\hat\beta_1$ dollars per hour *for each additional year of education*. 

### Optimization by Hand Demo

The following code very closely replicates the method used in the $lm$ function for minimizing the sum of squared residuals.

```{r optimDemo}
ssr <- function(b, y, x1, x2) {
  b1 <- b[1]
  b2 <- b[2]
  b0 <- b[3]
  sum((y - b0 - b1 * x1 - b2 * x2) ^ 2)
}
b.ols <-
  optim(
    par = c(mean(wage1$wage), 0, 0),
    fn = ssr,
    method = "BFGS",
    y = wage1$wage,
    x1 = wage1$educ,
    x2 = wage1$exper
  )
b.ols$par
sqrt(b.ols$value / (length(wage1$wage) - length(b.ols$par)))
```

Note: the value of the function at the minimum is the SSR, so $\hat\sigma^2 = b.ols\$value/(n-k)$

\newpage 
## Partition Regression

### Partialing Out Control Variables

Regress wages on experience and then education on experience. 

Name the results $wage.p$ and $educ.p$

```{r partials-solution, exercise.reveal_solution = FALSE}
wage.p <- lm(wage~exper, data = wage1)
educ.p <- lm(educ~exper, data = wage1)
```

### Regressing the Residuals

Summarize the regression of the residuals of these regressions on each other and compare the effects of education to the summary of $wage.lm2$. 

```{r residReg-solution, exercise.reveal_solution = FALSE}
summary(lm(wage.p$resid~educ.p$residuals))
summary(wage.lm2)
```

Notice that the coefficients are identical! The standard error of regression (and hence the standard errors of the coeffients, t-statistics, and p-values) differ slightly. This is because R only takes into account the education variable when determining the degrees of freedom. The true standard error of the residual regression should take that into account and the SER should be scaled up by $(n-2)/(n-k)$, where $k = 3$ in this example.

## Properties of OLS Redux

- $\sum_{i=1}^{n}{\hat{u}_i = 0}$
- $\sum_{i=1}^{n}{x_i\hat{u_i} = 0}$
- The *Total* Sum of Squares, $SST = \sum_{i=1}^n{(y_i-\bar{y})^2}$
- The *Explained* Sum of Squares, $SSE = \sum_{i=1}^n{(\hat{y}_i-\bar{y})^2}$
- The *Residual* Sum of Squares, $SSR = \sum_{i=1}^n{(y_i-\hat{y})^2}$
- SST = SSE + SSR (see section 2.3 for proof)
- Goodness of Fit, $R^2 = SSE/SST = 1 - SSR/SST$

We will discuss issues with using the (unadjusted) $R^2$ more in Chapter 6. 
